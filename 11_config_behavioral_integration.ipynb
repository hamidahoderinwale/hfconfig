{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config-Behavioral Integration\n",
        "\n",
        "**Goal**: Integrate config-based similarity with behavioral/capability data (LMArena) to analyze genotype-phenotype relationships.\n",
        "\n",
        "**Key Questions**:\n",
        "1. Does config drift predict behavioral drift?\n",
        "2. Which architectural features predict capability (Elo)?\n",
        "3. Do architecture clusters align with behavioral clusters?\n",
        "4. Are ecosystem-successful models also behaviorally successful?\n",
        "5. Are some families more behaviorally coherent than others?\n",
        "\n",
        "**Contents**:\n",
        "- LMArena data loading and model name mapping\n",
        "- Behavioral drift analysis (config change → capability change)\n",
        "- Architecture-capability regression (predicting Elo from config)\n",
        "- Architecture vs behavioral cluster comparison\n",
        "- Ecosystem fitness vs behavioral fitness analysis\n",
        "\n",
        "**Dependencies**: Uses Gower distance function. Requires LMArena data (CSV/JSON) for full functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Style matching main repo\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config data\n",
        "df = pd.read_csv('data/model_configs_expanded.csv', low_memory=False)\n",
        "print(f\"Loaded {len(df):,} models with config.json\")\n",
        "print(f\"Total columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Feature Preparation and Gower Distance\n",
        "\n",
        "**Note**: This section redefines utilities for self-contained execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features and categorize\n",
        "architecture_features = [\n",
        "    'config_model_type', 'config_hidden_size', 'config_num_hidden_layers',\n",
        "    'config_num_attention_heads', 'config_intermediate_size'\n",
        "]\n",
        "capacity_features = [\n",
        "    'config_vocab_size', 'config_max_position_embeddings', 'config_num_key_value_heads'\n",
        "]\n",
        "precision_features = [\n",
        "    'config_torch_dtype', 'config_rope_theta', 'config_rope_scaling_type'\n",
        "]\n",
        "boolean_features = ['uses_moe', 'uses_gqa', 'uses_rope', 'uses_quantization']\n",
        "\n",
        "all_features = architecture_features + capacity_features + precision_features + boolean_features\n",
        "available_features = [f for f in all_features if f in df.columns]\n",
        "\n",
        "# Categorize features\n",
        "numeric_features = []\n",
        "categorical_features = []\n",
        "boolean_feature_list = []\n",
        "\n",
        "for feat in available_features:\n",
        "    if feat in df.columns:\n",
        "        sample_values = df[feat].dropna().head(100)\n",
        "        if len(sample_values) == 0:\n",
        "            continue\n",
        "        try:\n",
        "            pd.to_numeric(sample_values, errors='raise')\n",
        "            numeric_features.append(feat)\n",
        "        except (ValueError, TypeError):\n",
        "            unique_vals = sample_values.unique()\n",
        "            if len(unique_vals) <= 2 and set(str(v).lower() for v in unique_vals).issubset({'true', 'false', '1', '0', 'yes', 'no', 'nan'}):\n",
        "                boolean_feature_list.append(feat)\n",
        "            else:\n",
        "                categorical_features.append(feat)\n",
        "\n",
        "print(f\"Features prepared: {len(available_features)} total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gower distance function (needed for config drift computation)\n",
        "def gower_distance(x, y, numeric_cols, categorical_cols, boolean_cols):\n",
        "    \"\"\"Compute Gower distance between two config vectors.\"\"\"\n",
        "    distance = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val, y_val = x[col], y[col]\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            try:\n",
        "                x_num, y_num = float(x_val), float(y_val)\n",
        "                max_val = max(abs(x_num), abs(y_num))\n",
        "                if max_val > 0:\n",
        "                    distance += abs(x_num - y_num) / max_val\n",
        "                count += 1\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val, y_val = x[col], y[col]\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            if str(x_val) != str(y_val):\n",
        "                distance += 1.0\n",
        "            count += 1\n",
        "    \n",
        "    for col in boolean_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val, y_val = x[col], y[col]\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            x_bool = bool(x_val) if not pd.isna(x_val) else False\n",
        "            y_bool = bool(y_val) if not pd.isna(y_val) else False\n",
        "            if x_bool != y_bool:\n",
        "                distance += 1.0\n",
        "            count += 1\n",
        "    \n",
        "    return distance / count if count > 0 else 1.0\n",
        "\n",
        "print(\"✓ Gower distance function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Config Drift Data (from notebook 09)\n",
        "\n",
        "**Note**: If you've run notebook 09, you can load the drift data. Otherwise, this notebook will compute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load pre-computed drift data, or compute it if not available\n",
        "import os\n",
        "if os.path.exists('config_drift_pairs.csv'):\n",
        "    df_drift = pd.read_csv('config_drift_pairs.csv')\n",
        "    print(f\"✓ Loaded pre-computed drift data: {len(df_drift):,} pairs\")\n",
        "else:\n",
        "    print(\"⚠ config_drift_pairs.csv not found\")\n",
        "    print(\"  This notebook can still run behavioral analyses, but config drift\")\n",
        "    print(\"  will need to be computed (see notebook 09 for drift computation)\")\n",
        "    df_drift = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prepare Config Features for Similarity Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder for behavioral drift analysis\n",
        "# This requires joining config data with behavioral evaluation datasets\n",
        "\n",
        "print(\"Behavioral drift analysis (placeholder)\")\n",
        "print(\"\\\\nThis analysis requires:\")\n",
        "print(\"  1. LMArena Elo scores or Arena-Hard-Auto scores\")\n",
        "print(\"  2. P2L (Prompt-to-Label) performance vectors\")\n",
        "print(\"  3. Joining behavioral data with config drift data\")\n",
        "print(\"\\\\nWhen behavioral data is available, this section will:\")\n",
        "print(\"  - Compute Δ_behavior = Elo(child) - Elo(parent)\")\n",
        "print(\"  - Regress capability change on config drift\")\n",
        "print(\"  - Identify which config changes correlate with capability changes\")\n",
        "print(\"  - Analyze prompt-specific behavioral drift (P2L clusters)\")\n",
        "\n",
        "# Example structure for when data is available:\n",
        "# behavioral_data = {\n",
        "#     'modelId': [...],\n",
        "#     'elo_score': [...],\n",
        "#     'arena_hard_score': [...],\n",
        "#     'p2l_vector': [...]\n",
        "# }\n",
        "# \n",
        "# df_behavioral = pd.DataFrame(behavioral_data)\n",
        "# df_drift_with_behavior = df_drift.merge(df_behavioral, left_on='child_id', right_on='modelId', how='inner')\n",
        "# \n",
        "# # Compute behavioral change\n",
        "# df_drift_with_behavior = df_drift_with_behavior.merge(\n",
        "#     df_behavioral, left_on='parent_id', right_on='modelId', \n",
        "#     suffixes=('_child', '_parent'), how='inner'\n",
        "# )\n",
        "# df_drift_with_behavior['delta_elo'] = df_drift_with_behavior['elo_score_child'] - df_drift_with_behavior['elo_score_parent']\n",
        "# \n",
        "# # Regression: capability change ~ config drift\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# X = df_drift_with_behavior[['drift']].values\n",
        "# y = df_drift_with_behavior['delta_elo'].values\n",
        "# model = LinearRegression().fit(X, y)\n",
        "# \n",
        "# print(f\\\"Regression: Δ_Elo ~ Config_Drift\\\")\n",
        "# print(f\\\"  Coefficient: {model.coef_[0]:.3f}\\\")\n",
        "# print(f\\\"  R²: {model.score(X, y):.3f}\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map LMArena model names to HuggingFace model IDs\n",
        "# This is a critical step - Arena uses display names, HF uses repo paths\n",
        "\n",
        "if lmarena_data is not None:\n",
        "    print(\"Mapping LMArena model names to HuggingFace modelIds...\")\n",
        "    \n",
        "    # Common mapping patterns\n",
        "    def normalize_model_name(name):\n",
        "        \"\"\"Normalize model name for matching\"\"\"\n",
        "        if pd.isna(name):\n",
        "            return None\n",
        "        name = str(name).lower().strip()\n",
        "        # Remove common prefixes/suffixes\n",
        "        name = name.replace(' (chat)', '').replace(' (instruct)', '').replace(' (base)', '')\n",
        "        name = name.replace('chat-', '').replace('instruct-', '').replace('base-', '')\n",
        "        return name\n",
        "    \n",
        "    # Normalize Arena names\n",
        "    lmarena_data['normalized_name'] = lmarena_data['model'].apply(normalize_model_name) if 'model' in lmarena_data.columns else None\n",
        "    \n",
        "    # Normalize HF modelIds\n",
        "    df['normalized_id'] = df['modelId'].apply(lambda x: normalize_model_name(x.split('/')[-1] if '/' in str(x) else str(x)))\n",
        "    \n",
        "    # Try direct matches first\n",
        "    if 'normalized_name' in lmarena_data.columns:\n",
        "        # Merge on normalized names\n",
        "        df_with_elo = df.merge(\n",
        "            lmarena_data[['model', 'elo_rating', 'elo_uncertainty']].rename(columns={'model': 'arena_model'}),\n",
        "            left_on='normalized_id',\n",
        "            right_on=lmarena_data['normalized_name'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Also try matching on modelId directly\n",
        "        direct_matches = df.merge(\n",
        "            lmarena_data[['model', 'elo_rating', 'elo_uncertainty']].rename(columns={'model': 'arena_model'}),\n",
        "            left_on='modelId',\n",
        "            right_on='model',\n",
        "            how='left',\n",
        "            suffixes=('', '_direct')\n",
        "        )\n",
        "        \n",
        "        # Combine matches\n",
        "        df['elo_rating'] = df_with_elo['elo_rating'].fillna(direct_matches.get('elo_rating', None))\n",
        "        df['elo_uncertainty'] = df_with_elo['elo_uncertainty'].fillna(direct_matches.get('elo_uncertainty', None))\n",
        "        df['arena_model'] = df_with_elo['arena_model'].fillna(direct_matches.get('arena_model', None))\n",
        "    \n",
        "    models_with_elo = df['elo_rating'].notna().sum()\n",
        "    print(f\"✓ Mapped {models_with_elo:,} models with Elo scores\")\n",
        "    print(f\"  Coverage: {models_with_elo/len(df)*100:.1f}% of config dataset\")\n",
        "    \n",
        "    if models_with_elo > 0:\n",
        "        print(f\"\\\\nElo score statistics:\")\n",
        "        print(f\"  Mean: {df['elo_rating'].mean():.1f}\")\n",
        "        print(f\"  Median: {df['elo_rating'].median():.1f}\")\n",
        "        print(f\"  Range: [{df['elo_rating'].min():.1f}, {df['elo_rating'].max():.1f}]\")\n",
        "else:\n",
        "    print(\"Skipping mapping - no LMArena data available\")\n",
        "    df['elo_rating'] = None\n",
        "    df['elo_uncertainty'] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Behavioral Drift: Config Change → Capability Change\n",
        "\n",
        "**Goal**: Analyze how config changes correlate with behavioral/capability changes (Elo scores from LMArena).\n",
        "\n",
        "**Key Questions**:\n",
        "- Does config drift predict behavioral drift?\n",
        "- Which architectural changes correlate with capability improvements?\n",
        "- Are some families more behaviorally coherent than others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute behavioral drift for parent-child pairs with Elo data\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 0:\n",
        "    print(\"Computing behavioral drift (ΔElo) for parent-child pairs...\")\n",
        "    \n",
        "    # Merge Elo scores into drift dataframe\n",
        "    df_drift_with_behavior = df_drift.merge(\n",
        "        df[['modelId', 'elo_rating', 'elo_uncertainty']].rename(columns={\n",
        "            'modelId': 'child_id',\n",
        "            'elo_rating': 'child_elo',\n",
        "            'elo_uncertainty': 'child_elo_uncertainty'\n",
        "        }),\n",
        "        on='child_id',\n",
        "        how='left'\n",
        "    ).merge(\n",
        "        df[['modelId', 'elo_rating', 'elo_uncertainty']].rename(columns={\n",
        "            'modelId': 'parent_id',\n",
        "            'elo_rating': 'parent_elo',\n",
        "            'elo_uncertainty': 'parent_elo_uncertainty'\n",
        "        }),\n",
        "        on='parent_id',\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Compute behavioral drift (ΔElo)\n",
        "    df_drift_with_behavior['delta_elo'] = (\n",
        "        df_drift_with_behavior['child_elo'] - df_drift_with_behavior['parent_elo']\n",
        "    )\n",
        "    \n",
        "    # Filter to pairs where both have Elo scores\n",
        "    df_behavioral_drift = df_drift_with_behavior[\n",
        "        df_drift_with_behavior['child_elo'].notna() & \n",
        "        df_drift_with_behavior['parent_elo'].notna()\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"\\\\n✓ Found {len(df_behavioral_drift):,} parent-child pairs with behavioral data\")\n",
        "    print(f\"  Mean ΔElo: {df_behavioral_drift['delta_elo'].mean():.2f}\")\n",
        "    print(f\"  Median ΔElo: {df_behavioral_drift['delta_elo'].median():.2f}\")\n",
        "    print(f\"  Improving pairs: {(df_behavioral_drift['delta_elo'] > 0).sum():,} ({(df_behavioral_drift['delta_elo'] > 0).mean()*100:.1f}%)\")\n",
        "    print(f\"  Regressing pairs: {(df_behavioral_drift['delta_elo'] < 0).sum():,} ({(df_behavioral_drift['delta_elo'] < 0).mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Visualize config drift vs behavioral drift\n",
        "    if len(df_behavioral_drift) > 10:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Scatter: Config drift vs Behavioral drift (THE CORE PLOT)\n",
        "        axes[0,0].scatter(df_behavioral_drift['drift'], df_behavioral_drift['delta_elo'], \n",
        "                         alpha=0.5, s=30, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "        axes[0,0].axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
        "        axes[0,0].axvline(df_behavioral_drift['drift'].median(), color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
        "        axes[0,0].set_xlabel('Config Drift (Gower Distance)', fontsize=11)\n",
        "        axes[0,0].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[0,0].set_title('Config Drift vs Behavioral Drift (Genotype → Phenotype)', fontsize=13)\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add correlation coefficient\n",
        "        from scipy.stats import pearsonr\n",
        "        corr, p_val = pearsonr(df_behavioral_drift['drift'], df_behavioral_drift['delta_elo'])\n",
        "        axes[0,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                      transform=axes[0,0].transAxes, fontsize=10,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 2. Distribution of behavioral drift\n",
        "        axes[0,1].hist(df_behavioral_drift['delta_elo'], bins=50, color='coral', alpha=0.7, edgecolor='white')\n",
        "        axes[0,1].axvline(0, color='red', linestyle='--', linewidth=2, label='No change')\n",
        "        axes[0,1].axvline(df_behavioral_drift['delta_elo'].median(), color='blue', linestyle='--', linewidth=2, \n",
        "                         label=f'Median: {df_behavioral_drift[\\\"delta_elo\\\"].median():.1f}')\n",
        "        axes[0,1].set_xlabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[0,1].set_ylabel('Count', fontsize=11)\n",
        "        axes[0,1].set_title('Distribution of Behavioral Drift', fontsize=13)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # 3. Behavioral drift by family\n",
        "        if 'parent_family' in df_behavioral_drift.columns:\n",
        "            family_behavioral_drift = df_behavioral_drift.groupby('parent_family')['delta_elo'].agg(['mean', 'median', 'count']).sort_values('count', ascending=False).head(10)\n",
        "            \n",
        "            x_pos = np.arange(len(family_behavioral_drift))\n",
        "            width = 0.35\n",
        "            axes[1,0].bar(x_pos - width/2, family_behavioral_drift['mean'], width, label='Mean', color='steelblue', alpha=0.7)\n",
        "            axes[1,0].bar(x_pos + width/2, family_behavioral_drift['median'], width, label='Median', color='coral', alpha=0.7)\n",
        "            axes[1,0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "            axes[1,0].set_xticks(x_pos)\n",
        "            axes[1,0].set_xticklabels(family_behavioral_drift.index, rotation=45, ha='right')\n",
        "            axes[1,0].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "            axes[1,0].set_title('Behavioral Drift by Family (Top 10)', fontsize=13)\n",
        "            axes[1,0].legend()\n",
        "            axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # 4. Quadrant plot: Config drift vs Behavioral drift\n",
        "        median_config_drift = df_behavioral_drift['drift'].median()\n",
        "        median_behavioral_drift = df_behavioral_drift['delta_elo'].median()\n",
        "        \n",
        "        quadrants = {\n",
        "            'High Config, High Behavior': (df_behavioral_drift['drift'] > median_config_drift) & (df_behavioral_drift['delta_elo'] > median_behavioral_drift),\n",
        "            'High Config, Low Behavior': (df_behavioral_drift['drift'] > median_config_drift) & (df_behavioral_drift['delta_elo'] <= median_behavioral_drift),\n",
        "            'Low Config, High Behavior': (df_behavioral_drift['drift'] <= median_config_drift) & (df_behavioral_drift['delta_elo'] > median_behavioral_drift),\n",
        "            'Low Config, Low Behavior': (df_behavioral_drift['drift'] <= median_config_drift) & (df_behavioral_drift['delta_elo'] <= median_behavioral_drift)\n",
        "        }\n",
        "        \n",
        "        colors = {'High Config, High Behavior': 'green', 'High Config, Low Behavior': 'orange',\n",
        "                 'Low Config, High Behavior': 'blue', 'Low Config, Low Behavior': 'red'}\n",
        "        \n",
        "        for quad_name, mask in quadrants.items():\n",
        "            if mask.sum() > 0:\n",
        "                axes[1,1].scatter(df_behavioral_drift[mask]['drift'], df_behavioral_drift[mask]['delta_elo'],\n",
        "                                 alpha=0.5, s=30, label=quad_name, c=colors[quad_name], edgecolors='black', linewidth=0.5)\n",
        "        \n",
        "        axes[1,1].axhline(median_behavioral_drift, color='gray', linestyle='--', alpha=0.5)\n",
        "        axes[1,1].axvline(median_config_drift, color='gray', linestyle='--', alpha=0.5)\n",
        "        axes[1,1].set_xlabel('Config Drift', fontsize=11)\n",
        "        axes[1,1].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[1,1].set_title('Config vs Behavioral Drift Quadrants', fontsize=13)\n",
        "        axes[1,1].legend(fontsize=8)\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/config_vs_behavioral_drift.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save behavioral drift data\n",
        "        df_behavioral_drift.to_csv('config_behavioral_drift.csv', index=False)\n",
        "        print(\"\\\\n✓ Behavioral drift data saved to config_behavioral_drift.csv\")\n",
        "    else:\n",
        "        print(\"Not enough behavioral data for visualization\")\n",
        "else:\n",
        "    print(\"No behavioral data available - skipping behavioral drift analysis\")\n",
        "    df_behavioral_drift = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Architecture-Capability Regression\n",
        "\n",
        "**Goal**: Predict behavioral capability (Elo) from architectural features (config.json parameters).\n",
        "\n",
        "**Key Questions**:\n",
        "- Which architectural features are most predictive of capability?\n",
        "- Are there nonlinear effects or diminishing returns?\n",
        "- Do architecture clusters correspond to capability clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict Elo from config features\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 50:\n",
        "    print(\"Training models to predict Elo from config features...\")\n",
        "    \n",
        "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import r2_score, mean_squared_error\n",
        "    \n",
        "    # Prepare data\n",
        "    df_elo = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    # Prepare features (numeric only for regression)\n",
        "    feature_cols = numeric_features + boolean_feature_list\n",
        "    X_cols = [col for col in feature_cols if col in df_elo.columns]\n",
        "    \n",
        "    # Fill missing values\n",
        "    X = df_elo[X_cols].fillna(0)\n",
        "    y = df_elo['elo_rating'].values\n",
        "    \n",
        "    # Remove columns with no variance\n",
        "    X = X.loc[:, X.std() > 0]\n",
        "    \n",
        "    print(f\"\\\\nTraining on {len(X):,} models with {len(X.columns)} features\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train models\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
        "        'Linear Regression': LinearRegression()\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'r2': r2,\n",
        "            'rmse': rmse,\n",
        "            'y_pred': y_pred,\n",
        "            'y_test': y_test\n",
        "        }\n",
        "        \n",
        "        print(f\"\\\\n{name}:\")\n",
        "        print(f\"  R²: {r2:.3f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "    \n",
        "    # Feature importance (from Random Forest)\n",
        "    if 'Random Forest' in results:\n",
        "        rf_model = results['Random Forest']['model']\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(f\"\\\\nTop 10 most predictive architectural features:\")\n",
        "        print(feature_importance.head(10).to_string(index=False))\n",
        "        \n",
        "        # Visualize feature importance\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Feature importance\n",
        "        top_features = feature_importance.head(15)\n",
        "        axes[0,0].barh(range(len(top_features)), top_features['importance'], color='steelblue', alpha=0.7)\n",
        "        axes[0,0].set_yticks(range(len(top_features)))\n",
        "        axes[0,0].set_yticklabels(top_features['feature'], fontsize=9)\n",
        "        axes[0,0].invert_yaxis()\n",
        "        axes[0,0].set_xlabel('Feature Importance', fontsize=11)\n",
        "        axes[0,0].set_title('Top 15 Most Predictive Config Features for Elo', fontsize=13)\n",
        "        axes[0,0].grid(True, alpha=0.3, axis='x')\n",
        "        \n",
        "        # 2. Predicted vs Actual (Random Forest)\n",
        "        axes[0,1].scatter(results['Random Forest']['y_test'], results['Random Forest']['y_pred'], \n",
        "                         alpha=0.5, s=20, color='steelblue', edgecolors='black', linewidth=0.3)\n",
        "        min_val = min(results['Random Forest']['y_test'].min(), results['Random Forest']['y_pred'].min())\n",
        "        max_val = max(results['Random Forest']['y_test'].max(), results['Random Forest']['y_pred'].max())\n",
        "        axes[0,1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
        "        axes[0,1].set_xlabel('Actual Elo', fontsize=11)\n",
        "        axes[0,1].set_ylabel('Predicted Elo', fontsize=11)\n",
        "        axes[0,1].set_title(f'Predicted vs Actual Elo (R²={results[\\\"Random Forest\\\"][\\\"r2\\\"]:.3f})', fontsize=13)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Model comparison\n",
        "        model_names = list(results.keys())\n",
        "        r2_scores = [results[m]['r2'] for m in model_names]\n",
        "        axes[1,0].bar(range(len(model_names)), r2_scores, color=['steelblue', 'coral', 'seagreen'], alpha=0.7)\n",
        "        axes[1,0].set_xticks(range(len(model_names)))\n",
        "        axes[1,0].set_xticklabels(model_names)\n",
        "        axes[1,0].set_ylabel('R² Score', fontsize=11)\n",
        "        axes[1,0].set_title('Model Performance Comparison', fontsize=13)\n",
        "        axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "        for i, score in enumerate(r2_scores):\n",
        "            axes[1,0].text(i, score + 0.01, f'{score:.3f}', ha='center', fontsize=10)\n",
        "        \n",
        "        # 4. Residuals plot\n",
        "        residuals = results['Random Forest']['y_test'] - results['Random Forest']['y_pred']\n",
        "        axes[1,1].scatter(results['Random Forest']['y_pred'], residuals, alpha=0.5, s=20, color='coral', edgecolors='black', linewidth=0.3)\n",
        "        axes[1,1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "        axes[1,1].set_xlabel('Predicted Elo', fontsize=11)\n",
        "        axes[1,1].set_ylabel('Residuals', fontsize=11)\n",
        "        axes[1,1].set_title('Residuals Plot', fontsize=13)\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/architecture_capability_regression.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save feature importance\n",
        "        feature_importance.to_csv('config_feature_importance_elo.csv', index=False)\n",
        "        print(\"\\\\n✓ Feature importance saved to config_feature_importance_elo.csv\")\n",
        "    else:\n",
        "        print(\"Random Forest model not available for feature importance\")\n",
        "else:\n",
        "    print(\"Not enough Elo data for regression analysis (need >50 models)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Architecture vs Behavioral Clusters\n",
        "\n",
        "**Goal**: Compare clustering based on architecture (config) vs clustering based on behavior (Elo/capability).\n",
        "\n",
        "**Key Questions**:\n",
        "- Do architecturally similar models behave similarly?\n",
        "- Are families behaviorally monomorphic or polymorphic?\n",
        "- Do architecture clusters align with behavioral clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare architecture-based vs behavior-based clustering\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 50:\n",
        "    print(\"Comparing architecture-based vs behavior-based clustering...\")\n",
        "    \n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "    \n",
        "    # Prepare data with both config and Elo\n",
        "    df_cluster = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    if len(df_cluster) > 100:\n",
        "        # Sample for computational efficiency\n",
        "        df_cluster = df_cluster.sample(n=min(1000, len(df_cluster)), random_state=42)\n",
        "    \n",
        "    # Architecture features (config-based)\n",
        "    arch_features = [col for col in numeric_features + boolean_feature_list if col in df_cluster.columns]\n",
        "    X_arch = df_cluster[arch_features].fillna(0)\n",
        "    X_arch = X_arch.loc[:, X_arch.std() > 0]  # Remove zero-variance columns\n",
        "    \n",
        "    # Standardize\n",
        "    scaler_arch = StandardScaler()\n",
        "    X_arch_scaled = scaler_arch.fit_transform(X_arch)\n",
        "    \n",
        "    # Behavioral features (Elo-based, can extend to multi-dimensional)\n",
        "    X_behavior = df_cluster[['elo_rating']].values\n",
        "    \n",
        "    # Cluster architectures\n",
        "    n_clusters = min(10, len(df_cluster) // 20)\n",
        "    kmeans_arch = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    arch_clusters = kmeans_arch.fit_predict(X_arch_scaled)\n",
        "    \n",
        "    # Cluster behaviors (by Elo bins)\n",
        "    behavior_clusters = pd.cut(df_cluster['elo_rating'], bins=n_clusters, labels=False)\n",
        "    \n",
        "    # Compute cluster alignment\n",
        "    ari = adjusted_rand_score(arch_clusters, behavior_clusters)\n",
        "    nmi = normalized_mutual_info_score(arch_clusters, behavior_clusters)\n",
        "    \n",
        "    print(f\"\\\\nCluster Alignment Metrics:\")\n",
        "    print(f\"  Adjusted Rand Index: {ari:.3f} (1.0 = perfect alignment, 0.0 = random)\")\n",
        "    print(f\"  Normalized Mutual Information: {nmi:.3f} (1.0 = perfect alignment, 0.0 = independent)\")\n",
        "    \n",
        "    # Visualize clusters\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Use t-SNE for 2D embedding\n",
        "    print(\"\\\\nComputing t-SNE embeddings (this may take a minute)...\")\n",
        "    tsne_arch = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df_cluster)-1))\n",
        "    embedding_arch = tsne_arch.fit_transform(X_arch_scaled)\n",
        "    \n",
        "    # 1. Architecture clusters colored by cluster\n",
        "    scatter1 = axes[0,0].scatter(embedding_arch[:, 0], embedding_arch[:, 1], \n",
        "                                c=arch_clusters, cmap='tab10', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
        "    axes[0,0].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
        "    axes[0,0].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
        "    axes[0,0].set_title('Architecture-Based Clusters (Config Features)', fontsize=13)\n",
        "    plt.colorbar(scatter1, ax=axes[0,0])\n",
        "    \n",
        "    # 2. Architecture embedding colored by Elo\n",
        "    scatter2 = axes[0,1].scatter(embedding_arch[:, 0], embedding_arch[:, 1], \n",
        "                                c=df_cluster['elo_rating'], cmap='viridis', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
        "    axes[0,1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
        "    axes[0,1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
        "    axes[0,1].set_title('Architecture Space Colored by Elo (Behavior)', fontsize=13)\n",
        "    plt.colorbar(scatter2, ax=axes[0,1], label='Elo Rating')\n",
        "    \n",
        "    # 3. Elo distribution by architecture cluster\n",
        "    cluster_elo = pd.DataFrame({\n",
        "        'cluster': arch_clusters,\n",
        "        'elo': df_cluster['elo_rating'].values\n",
        "    })\n",
        "    cluster_elo_stats = cluster_elo.groupby('cluster')['elo'].agg(['mean', 'std', 'count'])\n",
        "    \n",
        "    x_pos = np.arange(len(cluster_elo_stats))\n",
        "    axes[1,0].bar(x_pos, cluster_elo_stats['mean'], yerr=cluster_elo_stats['std'], \n",
        "                  color='steelblue', alpha=0.7, capsize=5)\n",
        "    axes[1,0].set_xticks(x_pos)\n",
        "    axes[1,0].set_xticklabels([f'Cluster {i}' for i in cluster_elo_stats.index])\n",
        "    axes[1,0].set_ylabel('Mean Elo Rating', fontsize=11)\n",
        "    axes[1,0].set_title('Behavioral Capability by Architecture Cluster', fontsize=13)\n",
        "    axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add count labels\n",
        "    for i, (idx, row) in enumerate(cluster_elo_stats.iterrows()):\n",
        "        axes[1,0].text(i, row['mean'] + row['std'] + 5, f\\\"n={int(row['count'])}\\\", ha='center', fontsize=8)\n",
        "    \n",
        "    # 4. Cluster alignment confusion matrix\n",
        "    confusion = pd.crosstab(pd.Series(arch_clusters, name='Architecture Cluster'),\n",
        "                           pd.Series(behavior_clusters, name='Behavior Cluster'))\n",
        "    im = axes[1,1].imshow(confusion.values, cmap='YlOrRd', aspect='auto')\n",
        "    axes[1,1].set_xticks(range(len(confusion.columns)))\n",
        "    axes[1,1].set_xticklabels([f'B{i}' for i in confusion.columns])\n",
        "    axes[1,1].set_yticks(range(len(confusion.index)))\n",
        "    axes[1,1].set_yticklabels([f'A{i}' for i in confusion.index])\n",
        "    axes[1,1].set_xlabel('Behavior Cluster', fontsize=11)\n",
        "    axes[1,1].set_ylabel('Architecture Cluster', fontsize=11)\n",
        "    axes[1,1].set_title(f'Architecture vs Behavior Cluster Alignment\\\\n(ARI={ari:.3f}, NMI={nmi:.3f})', fontsize=13)\n",
        "    plt.colorbar(im, ax=axes[1,1], label='Count')\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(confusion.index)):\n",
        "        for j in range(len(confusion.columns)):\n",
        "            axes[1,1].text(j, i, str(confusion.iloc[i, j]), ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('figures/architecture_vs_behavior_clusters.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\n✓ Cluster analysis complete\")\n",
        "else:\n",
        "    print(\"Not enough behavioral data for cluster comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Ecosystem Fitness vs Behavioral Fitness\n",
        "\n",
        "**Goal**: Compare ecosystem success metrics (downloads, descendants, likes) with behavioral capability (Elo).\n",
        "\n",
        "**Key Questions**:\n",
        "- Are the most downloaded models the most capable?\n",
        "- Do high-performing models have more descendants?\n",
        "- What is the relationship between ecosystem adoption and behavioral performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare ecosystem fitness vs behavioral fitness\n",
        "# Try to load ecosystem metrics (downloads, likes, descendants)\n",
        "print(\"Analyzing ecosystem fitness vs behavioral fitness...\")\n",
        "\n",
        "# Check for ecosystem metrics in dataframe\n",
        "ecosystem_metrics = {}\n",
        "for metric in ['downloads', 'likes', 'num_descendants', 'num_children']:\n",
        "    if metric in df.columns:\n",
        "        ecosystem_metrics[metric] = df[metric]\n",
        "\n",
        "# Also try loading from graph if available\n",
        "if G_family is not None and 'elo_rating' in df.columns:\n",
        "    # Count descendants for models with Elo\n",
        "    df_with_descendants = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    def count_descendants_simple(model_id):\n",
        "        try:\n",
        "            descendants = nx.descendants(G_family, model_id)\n",
        "            return len(descendants)\n",
        "        except:\n",
        "            return 0\n",
        "    \n",
        "    # Sample for performance\n",
        "    if len(df_with_descendants) > 500:\n",
        "        df_with_descendants = df_with_descendants.sample(n=500, random_state=42)\n",
        "    \n",
        "    df_with_descendants['num_descendants'] = df_with_descendants['modelId'].apply(count_descendants_simple)\n",
        "    ecosystem_metrics['num_descendants'] = df_with_descendants.set_index('modelId')['num_descendants']\n",
        "\n",
        "if len(ecosystem_metrics) > 0 and 'elo_rating' in df.columns:\n",
        "    # Merge ecosystem metrics with Elo\n",
        "    df_fitness = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    for metric_name, metric_series in ecosystem_metrics.items():\n",
        "        if isinstance(metric_series, pd.Series):\n",
        "            df_fitness = df_fitness.merge(\n",
        "                metric_series.reset_index().rename(columns={metric_series.name: metric_name}),\n",
        "                left_on='modelId',\n",
        "                right_on=metric_series.index.name if metric_series.index.name else 'index',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_fitness[metric_name] = df_fitness['modelId'].map(metric_series).fillna(0)\n",
        "    \n",
        "    # Filter to models with both Elo and at least one ecosystem metric\n",
        "    has_metric = df_fitness[[m for m in ecosystem_metrics.keys()]].notna().any(axis=1)\n",
        "    df_fitness = df_fitness[has_metric].copy()\n",
        "    \n",
        "    if len(df_fitness) > 10:\n",
        "        print(f\"\\\\n✓ Analyzing {len(df_fitness):,} models with both Elo and ecosystem metrics\")\n",
        "        \n",
        "        # Visualize relationships\n",
        "        n_metrics = len(ecosystem_metrics)\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Scatter: Downloads vs Elo\n",
        "        if 'downloads' in df_fitness.columns:\n",
        "            # Log scale for downloads\n",
        "            log_downloads = np.log10(df_fitness['downloads'] + 1)\n",
        "            axes[0,0].scatter(log_downloads, df_fitness['elo_rating'], alpha=0.5, s=30, \n",
        "                            color='steelblue', edgecolors='black', linewidth=0.3)\n",
        "            axes[0,0].set_xlabel('Log10(Downloads + 1)', fontsize=11)\n",
        "            axes[0,0].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[0,0].set_title('Ecosystem Adoption (Downloads) vs Behavioral Capability', fontsize=13)\n",
        "            axes[0,0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add correlation\n",
        "            corr, p_val = pearsonr(log_downloads, df_fitness['elo_rating'])\n",
        "            axes[0,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                          transform=axes[0,0].transAxes, fontsize=10,\n",
        "                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 2. Quadrant plot: Ecosystem Fitness vs Behavioral Fitness\n",
        "        if 'downloads' in df_fitness.columns:\n",
        "            median_downloads = df_fitness['downloads'].median()\n",
        "            median_elo = df_fitness['elo_rating'].median()\n",
        "            \n",
        "            quadrants = {\n",
        "                'High Adoption, High Capability': (df_fitness['downloads'] > median_downloads) & (df_fitness['elo_rating'] > median_elo),\n",
        "                'High Adoption, Low Capability': (df_fitness['downloads'] > median_downloads) & (df_fitness['elo_rating'] <= median_elo),\n",
        "                'Low Adoption, High Capability': (df_fitness['downloads'] <= median_downloads) & (df_fitness['elo_rating'] > median_elo),\n",
        "                'Low Adoption, Low Capability': (df_fitness['downloads'] <= median_downloads) & (df_fitness['elo_rating'] <= median_elo)\n",
        "            }\n",
        "            \n",
        "            colors = {'High Adoption, High Capability': 'green', 'High Adoption, Low Capability': 'orange',\n",
        "                     'Low Adoption, High Capability': 'blue', 'Low Adoption, Low Capability': 'red'}\n",
        "            \n",
        "            for quad_name, mask in quadrants.items():\n",
        "                if mask.sum() > 0:\n",
        "                    axes[0,1].scatter(np.log10(df_fitness[mask]['downloads'] + 1), \n",
        "                                     df_fitness[mask]['elo_rating'],\n",
        "                                     alpha=0.5, s=30, label=quad_name, c=colors[quad_name], \n",
        "                                     edgecolors='black', linewidth=0.3)\n",
        "            \n",
        "            axes[0,1].axhline(median_elo, color='gray', linestyle='--', alpha=0.5)\n",
        "            axes[0,1].axvline(np.log10(median_downloads + 1), color='gray', linestyle='--', alpha=0.5)\n",
        "            axes[0,1].set_xlabel('Log10(Downloads + 1)', fontsize=11)\n",
        "            axes[0,1].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[0,1].set_title('Ecosystem Fitness vs Behavioral Fitness Quadrants', fontsize=13)\n",
        "            axes[0,1].legend(fontsize=8)\n",
        "            axes[0,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Descendants vs Elo\n",
        "        if 'num_descendants' in df_fitness.columns:\n",
        "            log_descendants = np.log10(df_fitness['num_descendants'] + 1)\n",
        "            axes[1,0].scatter(log_descendants, df_fitness['elo_rating'], alpha=0.5, s=30,\n",
        "                            color='coral', edgecolors='black', linewidth=0.3)\n",
        "            axes[1,0].set_xlabel('Log10(Number of Descendants + 1)', fontsize=11)\n",
        "            axes[1,0].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[1,0].set_title('Reproductive Success vs Behavioral Capability', fontsize=13)\n",
        "            axes[1,0].grid(True, alpha=0.3)\n",
        "            \n",
        "            corr, p_val = pearsonr(log_descendants, df_fitness['elo_rating'])\n",
        "            axes[1,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                          transform=axes[1,0].transAxes, fontsize=10,\n",
        "                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 4. Correlation matrix\n",
        "        fitness_cols = ['elo_rating'] + [m for m in ecosystem_metrics.keys() if m in df_fitness.columns]\n",
        "        fitness_data = df_fitness[fitness_cols].copy()\n",
        "        \n",
        "        # Log transform ecosystem metrics\n",
        "        for col in fitness_data.columns:\n",
        "            if col != 'elo_rating' and fitness_data[col].max() > 100:\n",
        "                fitness_data[col] = np.log10(fitness_data[col] + 1)\n",
        "        \n",
        "        corr_matrix = fitness_data.corr()\n",
        "        im = axes[1,1].imshow(corr_matrix.values, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "        axes[1,1].set_xticks(range(len(corr_matrix.columns)))\n",
        "        axes[1,1].set_yticks(range(len(corr_matrix.index)))\n",
        "        axes[1,1].set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=9)\n",
        "        axes[1,1].set_yticklabels(corr_matrix.index, fontsize=9)\n",
        "        axes[1,1].set_title('Correlation: Ecosystem Metrics vs Behavioral Capability', fontsize=13)\n",
        "        plt.colorbar(im, ax=axes[1,1], label='Correlation')\n",
        "        \n",
        "        # Add correlation values\n",
        "        for i in range(len(corr_matrix.index)):\n",
        "            for j in range(len(corr_matrix.columns)):\n",
        "                axes[1,1].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
        "                              ha='center', va='center', fontsize=8,\n",
        "                              color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/ecosystem_vs_behavioral_fitness.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save fitness data\n",
        "        df_fitness[['modelId', 'elo_rating'] + list(ecosystem_metrics.keys())].to_csv('ecosystem_behavioral_fitness.csv', index=False)\n",
        "        print(\"\\\\n✓ Fitness data saved to ecosystem_behavioral_fitness.csv\")\n",
        "    else:\n",
        "        print(\"Not enough data for fitness analysis\")\n",
        "else:\n",
        "    print(\"Ecosystem metrics or Elo data not available - skipping fitness analysis\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
