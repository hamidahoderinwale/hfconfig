{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architectural Fitness: Which Configs Produce More Descendants?\n",
        "\n",
        "**Goal**: Identify which architectural configurations are \"fittest\" in the ecosystem - i.e., which lead to more descendants, usage, or adoption.\n",
        "\n",
        "**Core Question**: Which architectures are \"fittest\" in the ecosystem ‚Äì i.e., which config patterns lead to more descendants, usage, or adoption?\n",
        "\n",
        "**Analysis Approach**:\n",
        "1. Compute architectural traits (hidden_size, num_layers, context length, dtype, model_type)\n",
        "2. Attach ecosystem stats (#children, #downloads, #likes)\n",
        "3. Regress \"#descendants\" or \"#downloads\" on architectural traits\n",
        "4. Compare \"fitness\" landscapes by architecture family\n",
        "5. Look for non-linear effects and sweet spots (e.g., 7B models being more \"reproductively fit\" than 13B or 70B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Style matching main repo\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 14,557 models with config.json\n",
            "Loading from HuggingFace dataset (modelbiome/ai_ecosystem)...\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load main dataset for ecosystem stats\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load from HuggingFace dataset which has all metadata columns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading from HuggingFace dataset (modelbiome/ai_ecosystem)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mmodelbiome/ai_ecosystem\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
          ]
        }
      ],
      "source": [
        "# Load config data\n",
        "df_configs = pd.read_csv('data/model_configs_expanded.csv', low_memory=False)\n",
        "print(f\"Loaded {len(df_configs):,} models with config.json\")\n",
        "\n",
        "# Load main dataset for ecosystem stats\n",
        "# Try to load from HuggingFace dataset (modelbiome/ai_ecosystem), fallback on local CSVs if datasets lib missing\n",
        "print(\"Loading from HuggingFace dataset (modelbiome/ai_ecosystem)...\")\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(\"ModuleNotFoundError: No module named 'datasets'\")\n",
        "    DATASETS_AVAILABLE = False\n",
        "\n",
        "if DATASETS_AVAILABLE:\n",
        "    try:\n",
        "    dataset = load_dataset(\"modelbiome/ai_ecosystem\", split=\"train\")\n",
        "    print(f\"Dataset loaded with {len(dataset):,} models\")\n",
        "    print(f\"Available columns: {dataset.column_names[:20]}...\")\n",
        "    \n",
        "    # Build DataFrame column by column, checking if each exists\n",
        "    data_dict = {'modelId': dataset['model_id']}\n",
        "    \n",
        "    # Columns we need for fitness analysis\n",
        "    numeric_cols = ['downloads', 'likes']\n",
        "    list_cols = ['parent_model', 'finetune_parent', 'quantized_parent', \n",
        "                 'adapter_parent', 'merge_parent']\n",
        "    \n",
        "    # Add numeric columns\n",
        "    for col in numeric_cols:\n",
        "        if col in dataset.column_names:\n",
        "            data_dict[col] = dataset[col]\n",
        "        else:\n",
        "            data_dict[col] = [0] * len(dataset)\n",
        "    \n",
        "    # Add list columns (parent relationships)\n",
        "    for col in list_cols:\n",
        "        if col in dataset.column_names:\n",
        "            data_dict[col] = dataset[col]\n",
        "        else:\n",
        "            data_dict[col] = [[]] * len(dataset)\n",
        "    \n",
        "    df_main = pd.DataFrame(data_dict)\n",
        "    print(f\"Loaded {len(df_main):,} models from HuggingFace\")\n",
        "    print(f\"Columns in df_main: {df_main.columns.tolist()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading from HuggingFace: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nTrying alternative: load from expanded dataset if available...\")\n",
        "    # Try to find an expanded dataset CSV\n",
        "    try:\n",
        "        import os\n",
        "        alt_paths = [\n",
        "            'data/ai_ecosystem_expanded.csv',\n",
        "            'data/ai_ecosystem.csv',\n",
        "            '../ai_ecosystem.csv'\n",
        "        ]\n",
        "        found = False\n",
        "        for path in alt_paths:\n",
        "            if os.path.exists(path):\n",
        "                df_main = pd.read_csv(path, low_memory=False)\n",
        "                if 'modelId' not in df_main.columns and 'model_id' in df_main.columns:\n",
        "                    df_main['modelId'] = df_main['model_id']\n",
        "                # Ensure required columns exist\n",
        "                for col in ['downloads', 'likes', 'parent_model', 'finetune_parent']:\n",
        "                    if col not in df_main.columns:\n",
        "                        if col in ['parent_model', 'finetune_parent']:\n",
        "                            df_main[col] = [[]] * len(df_main)\n",
        "                        else:\n",
        "                            df_main[col] = 0\n",
        "                print(f\"Loaded {len(df_main):,} models from {path}\")\n",
        "                found = True\n",
        "                break\n",
        "        \n",
        "        if not found:\n",
        "            # Last resort: create minimal dataframe\n",
        "            df_main = pd.DataFrame({'modelId': df_configs['modelId']})\n",
        "            df_main['downloads'] = 0\n",
        "            df_main['likes'] = 0\n",
        "            df_main['parent_model'] = [[]] * len(df_main)\n",
        "            df_main['finetune_parent'] = [[]] * len(df_main)\n",
        "            print(\"Created minimal dataframe - fitness analysis will use graph data\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Error in fallback: {e2}\")\n",
        "        df_main = pd.DataFrame({'modelId': df_configs['modelId']})\n",
        "        df_main['downloads'] = 0\n",
        "        df_main['likes'] = 0\n",
        "        df_main['parent_model'] = [[]] * len(df_main)\n",
        "        df_main['finetune_parent'] = [[]] * len(df_main)\n",
        "\n",
        "# Join - only use columns that exist\n",
        "cols_to_merge = ['modelId']\n",
        "for col in ['downloads', 'likes', 'parent_model', 'finetune_parent']:\n",
        "    if col in df_main.columns:\n",
        "        cols_to_merge.append(col)\n",
        "\n",
        "df = df_configs.merge(df_main[cols_to_merge], on='modelId', how='left')\n",
        "print(f\"\\nJoined dataset: {len(df):,} models\")\n",
        "if 'downloads' in df.columns:\n",
        "    print(f\"Models with downloads data: {df['downloads'].notna().sum():,}\")\n",
        "    print(f\"Total downloads: {df['downloads'].sum():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count descendants - try graph first, then fallback to parent_model counts\n",
        "import pickle\n",
        "import networkx as nx\n",
        "\n",
        "def count_descendants_from_graph(model_id, G):\n",
        "    \"\"\"Count all descendants using graph traversal\"\"\"\n",
        "    if model_id in G:\n",
        "        descendants = set()\n",
        "        to_process = [model_id]\n",
        "        while to_process:\n",
        "            current = to_process.pop()\n",
        "            if current in G:\n",
        "                for child in G.successors(current):\n",
        "                    if child not in descendants:\n",
        "                        descendants.add(child)\n",
        "                        to_process.append(child)\n",
        "        return len(descendants)\n",
        "    return 0\n",
        "\n",
        "def count_descendants_from_parents(df_with_parents):\n",
        "    \"\"\"Count direct children from parent_model lists\"\"\"\n",
        "    parent_counts = {}\n",
        "    for idx, row in df_with_parents.iterrows():\n",
        "        parents = row.get('parent_model', [])\n",
        "        # Handle both string representation of lists and actual lists\n",
        "        if isinstance(parents, str):\n",
        "            try:\n",
        "                import ast\n",
        "                parents = ast.literal_eval(parents)\n",
        "            except:\n",
        "                parents = []\n",
        "        if isinstance(parents, list):\n",
        "            for parent in parents:\n",
        "                if parent:  # Skip empty strings\n",
        "                    parent_counts[parent] = parent_counts.get(parent, 0) + 1\n",
        "    return parent_counts\n",
        "\n",
        "try:\n",
        "    with open('data/ai_ecosystem_graph_finetune_fulljson.pkl', 'rb') as f:\n",
        "        G = pickle.load(f)\n",
        "    print(f\"Loaded graph with {len(G.nodes):,} nodes\")\n",
        "    \n",
        "    print(\"Counting descendants from graph (this may take a few minutes)...\")\n",
        "    df['num_descendants'] = df['modelId'].apply(lambda x: count_descendants_from_graph(x, G))\n",
        "    print(\"‚úì Descendant counts computed from graph\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Could not load graph: {e}\")\n",
        "    print(\"Using parent_model counts as proxy...\")\n",
        "    \n",
        "    # Fallback: count direct children from parent_model column\n",
        "    if 'parent_model' in df.columns:\n",
        "        parent_counts = count_descendants_from_parents(df)\n",
        "        df['num_descendants'] = df['modelId'].map(parent_counts).fillna(0)\n",
        "        print(f\"‚úì Using parent counts as proxy: {df['num_descendants'].sum():,} total parent relationships\")\n",
        "    else:\n",
        "        print(\"No parent_model column available - setting descendants to 0\")\n",
        "        df['num_descendants'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare architectural features\n",
        "df['hidden_size'] = pd.to_numeric(df['config_hidden_size'], errors='coerce')\n",
        "df['num_layers'] = pd.to_numeric(df['config_num_hidden_layers'], errors='coerce')\n",
        "df['context_length'] = pd.to_numeric(df['config_max_position_embeddings'], errors='coerce')\n",
        "df['approx_params'] = pd.to_numeric(df['config_approx_params_billions'], errors='coerce')\n",
        "\n",
        "# Log transforms for regression\n",
        "df['log_hidden_size'] = np.log1p(df['hidden_size'])\n",
        "df['log_num_layers'] = np.log1p(df['num_layers'])\n",
        "df['log_context_length'] = np.log1p(df['context_length'])\n",
        "df['log_params'] = np.log1p(df['approx_params'])\n",
        "df['log_downloads'] = np.log1p(df['downloads'])\n",
        "df['log_descendants'] = np.log1p(df['num_descendants'])\n",
        "\n",
        "# Model family\n",
        "family_cols = [c for c in df.columns if c.startswith('is_') and 'family' in c]\n",
        "df['model_family'] = 'other'\n",
        "for col in family_cols:\n",
        "    family_name = col.replace('is_', '').replace('_family', '')\n",
        "    mask = ((df[col] == True) | (df[col] == 'True'))\n",
        "    df.loc[mask, 'model_family'] = family_name\n",
        "\n",
        "print(f\"\\nModels by family:\")\n",
        "print(df['model_family'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Fitness Landscape: Downloads vs Architectural Traits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to models with valid data\n",
        "df_fit = df[\n",
        "    (df['hidden_size'].notna()) & \n",
        "    (df['num_layers'].notna()) &\n",
        "    (df['downloads'].notna()) &\n",
        "    (df['downloads'] > 0)\n",
        "].copy()\n",
        "\n",
        "print(f\"Models with complete data: {len(df_fit):,}\")\n",
        "\n",
        "# Regression: downloads ~ architectural traits\n",
        "features = ['log_hidden_size', 'log_num_layers', 'log_context_length', 'log_params']\n",
        "X = df_fit[features].fillna(0)\n",
        "y = df_fit['log_downloads']\n",
        "\n",
        "# Remove rows with any NaN\n",
        "valid_mask = ~X.isna().any(axis=1) & ~y.isna()\n",
        "X_clean = X[valid_mask]\n",
        "y_clean = y[valid_mask]\n",
        "\n",
        "if len(X_clean) > 100:\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_clean)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_scaled, y_clean)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"REGRESSION: log(downloads) ~ Architectural Traits\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nR¬≤ Score: {model.score(X_scaled, y_clean):.4f}\")\n",
        "    print(f\"\\nCoefficients:\")\n",
        "    for feat, coef in zip(features, model.coef_):\n",
        "        print(f\"  {feat}: {coef:.4f}\")\n",
        "    print(f\"\\nIntercept: {model.intercept_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fitness by Model Size (Parameter Count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by approximate parameter count bins\n",
        "df_fit['param_bin'] = pd.cut(df_fit['approx_params'], \n",
        "                              bins=[0, 1, 3, 7, 13, 30, 70, 200, np.inf],\n",
        "                              labels=['<1B', '1-3B', '3-7B', '7-13B', '13-30B', '30-70B', '70-200B', '>200B'])\n",
        "\n",
        "# Calculate average fitness metrics by bin\n",
        "fitness_by_size = df_fit.groupby('param_bin').agg({\n",
        "    'num_descendants': ['mean', 'median', 'count'],\n",
        "    'downloads': ['mean', 'median'],\n",
        "    'likes': ['mean', 'median']\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FITNESS BY MODEL SIZE\")\n",
        "print(\"=\"*80)\n",
        "print(fitness_by_size)\n",
        "\n",
        "# Plot fitness landscape\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Average descendants by size\n",
        "size_order = ['<1B', '1-3B', '3-7B', '7-13B', '13-30B', '30-70B', '70-200B', '>200B']\n",
        "size_data = df_fit[df_fit['param_bin'].isin(size_order)]\n",
        "\n",
        "desc_by_size = size_data.groupby('param_bin')['num_descendants'].mean()\n",
        "axes[0].bar(range(len(desc_by_size)), desc_by_size.values, color=plt.cm.viridis(np.linspace(0, 1, len(desc_by_size))))\n",
        "axes[0].set_xticks(range(len(desc_by_size)))\n",
        "axes[0].set_xticklabels(desc_by_size.index, rotation=45, ha='right')\n",
        "axes[0].set_ylabel('Average # Descendants')\n",
        "axes[0].set_title('Reproductive Fitness by Model Size')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Average downloads by size\n",
        "dl_by_size = size_data.groupby('param_bin')['downloads'].mean()\n",
        "axes[1].bar(range(len(dl_by_size)), dl_by_size.values, color=plt.cm.plasma(np.linspace(0, 1, len(dl_by_size))))\n",
        "axes[1].set_xticks(range(len(dl_by_size)))\n",
        "axes[1].set_xticklabels(dl_by_size.index, rotation=45, ha='right')\n",
        "axes[1].set_ylabel('Average Downloads')\n",
        "axes[1].set_title('Usage Fitness by Model Size')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/fitness_by_size.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fitness by Architecture Family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare fitness across families\n",
        "main_families = ['llama', 'mistral', 'qwen', 'gpt', 'bert', 'deepseek']\n",
        "df_family = df_fit[df_fit['model_family'].isin(main_families)].copy()\n",
        "\n",
        "family_fitness = df_family.groupby('model_family').agg({\n",
        "    'num_descendants': ['mean', 'median', 'count'],\n",
        "    'downloads': ['mean', 'median'],\n",
        "    'likes': ['mean', 'median'],\n",
        "    'approx_params': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FITNESS BY ARCHITECTURE FAMILY\")\n",
        "print(\"=\"*80)\n",
        "print(family_fitness)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "family_order = family_fitness.sort_values(('num_descendants', 'mean'), ascending=False).index\n",
        "\n",
        "desc_means = [family_fitness.loc[f, ('num_descendants', 'mean')] for f in family_order]\n",
        "axes[0].barh(range(len(family_order)), desc_means, color=plt.cm.Set3(np.linspace(0, 1, len(family_order))))\n",
        "axes[0].set_yticks(range(len(family_order)))\n",
        "axes[0].set_yticklabels([f.title() for f in family_order])\n",
        "axes[0].set_xlabel('Average # Descendants')\n",
        "axes[0].set_title('Reproductive Fitness by Family')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "dl_means = [family_fitness.loc[f, ('downloads', 'mean')] for f in family_order]\n",
        "axes[1].barh(range(len(family_order)), dl_means, color=plt.cm.Set2(np.linspace(0, 1, len(family_order))))\n",
        "axes[1].set_yticks(range(len(family_order)))\n",
        "axes[1].set_yticklabels([f.title() for f in family_order])\n",
        "axes[1].set_xlabel('Average Downloads')\n",
        "axes[1].set_title('Usage Fitness by Family')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/fitness_by_family.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Context Length Fitness by Family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare returns to context length by family\n",
        "df_ctx = df_family[df_family['context_length'].notna()].copy()\n",
        "df_ctx['ctx_bin'] = pd.cut(df_ctx['context_length'], \n",
        "                           bins=[0, 2048, 8192, 32768, 131072, np.inf],\n",
        "                           labels=['<2K', '2-8K', '8-32K', '32-128K', '>128K'])\n",
        "\n",
        "# Plot context length vs fitness by family\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for family in ['llama', 'mistral', 'qwen']:\n",
        "    family_df = df_ctx[df_ctx['model_family'] == family]\n",
        "    if len(family_df) > 10:\n",
        "        ctx_fitness = family_df.groupby('ctx_bin')['num_descendants'].mean()\n",
        "        axes[0].plot(range(len(ctx_fitness)), ctx_fitness.values, \n",
        "                    marker='o', label=family.title(), linewidth=2, markersize=6)\n",
        "        \n",
        "        ctx_dl = family_df.groupby('ctx_bin')['downloads'].mean()\n",
        "        axes[1].plot(range(len(ctx_dl)), ctx_dl.values, \n",
        "                    marker='o', label=family.title(), linewidth=2, markersize=6)\n",
        "\n",
        "axes[0].set_xticks(range(len(ctx_fitness)))\n",
        "axes[0].set_xticklabels(ctx_fitness.index, rotation=45, ha='right')\n",
        "axes[0].set_ylabel('Average # Descendants')\n",
        "axes[0].set_title('Context Length Fitness: Descendants')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].set_xticks(range(len(ctx_dl)))\n",
        "axes[1].set_xticklabels(ctx_dl.index, rotation=45, ha='right')\n",
        "axes[1].set_ylabel('Average Downloads')\n",
        "axes[1].set_title('Context Length Fitness: Downloads')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/fitness_context_by_family.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sweet Spot Analysis: Non-Linear Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look for sweet spots in parameter count\n",
        "# Focus on common sizes: 1B, 3B, 7B, 13B, 30B, 70B\n",
        "sweet_spots = [1, 3, 7, 13, 30, 70]\n",
        "\n",
        "df_sweet = df_fit[\n",
        "    (df_fit['approx_params'].notna()) & \n",
        "    (df_fit['approx_params'] > 0.5) & \n",
        "    (df_fit['approx_params'] < 200)\n",
        "].copy()\n",
        "\n",
        "# Bin by proximity to sweet spots\n",
        "def find_nearest_sweet_spot(params):\n",
        "    if pd.isna(params):\n",
        "        return None\n",
        "    nearest = min(sweet_spots, key=lambda x: abs(x - params))\n",
        "    return f\"~{nearest}B\"\n",
        "\n",
        "df_sweet['sweet_spot'] = df_sweet['approx_params'].apply(find_nearest_sweet_spot)\n",
        "\n",
        "# Calculate fitness by sweet spot\n",
        "sweet_fitness = df_sweet.groupby('sweet_spot').agg({\n",
        "    'num_descendants': ['mean', 'median', 'count'],\n",
        "    'downloads': ['mean', 'median']\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SWEET SPOT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(sweet_fitness)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "spot_order = [f\"~{s}B\" for s in sweet_spots]\n",
        "spot_order = [s for s in spot_order if s in sweet_fitness.index]\n",
        "\n",
        "desc_means = [sweet_fitness.loc[s, ('num_descendants', 'mean')] for s in spot_order]\n",
        "ax.plot(range(len(spot_order)), desc_means, marker='o', markersize=10, \n",
        "        linewidth=3, label='Average Descendants', color='#2E86AB')\n",
        "\n",
        "ax.set_xticks(range(len(spot_order)))\n",
        "ax.set_xticklabels(spot_order)\n",
        "ax.set_ylabel('Average # Descendants')\n",
        "ax.set_xlabel('Model Size (Approx. Parameters)')\n",
        "ax.set_title('Reproductive Fitness: Sweet Spot Analysis', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/fitness_sweet_spots.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Identify sweetest spot\n",
        "best_spot = sweet_fitness[('num_descendants', 'mean')].idxmax()\n",
        "best_fitness = sweet_fitness.loc[best_spot, ('num_descendants', 'mean')]\n",
        "print(f\"\\nüèÜ Sweetest Spot: {best_spot} with {best_fitness:.1f} average descendants\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Impact on Fitness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare fitness for models with/without key features\n",
        "features_to_test = ['uses_moe', 'uses_gqa', 'uses_rope', 'uses_quantization']\n",
        "\n",
        "feature_impact = []\n",
        "for feat in features_to_test:\n",
        "    if feat in df_fit.columns:\n",
        "        df_feat = df_fit[df_fit[feat].isin([True, 'True', 1])]\n",
        "        df_no_feat = df_fit[df_fit[feat].isin([False, 'False', 0])]\n",
        "        \n",
        "        if len(df_feat) > 10 and len(df_no_feat) > 10:\n",
        "            feature_impact.append({\n",
        "                'feature': feat.replace('uses_', ''),\n",
        "                'with_feature_desc': df_feat['num_descendants'].mean(),\n",
        "                'without_feature_desc': df_no_feat['num_descendants'].mean(),\n",
        "                'with_feature_dl': df_feat['downloads'].mean(),\n",
        "                'without_feature_dl': df_no_feat['downloads'].mean(),\n",
        "                'n_with': len(df_feat),\n",
        "                'n_without': len(df_no_feat)\n",
        "            })\n",
        "\n",
        "impact_df = pd.DataFrame(feature_impact)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE IMPACT ON FITNESS\")\n",
        "print(\"=\"*80)\n",
        "print(impact_df.round(2))\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "x_pos = np.arange(len(impact_df))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x_pos - width/2, impact_df['with_feature_desc'], width, \n",
        "            label='With Feature', color='#2E86AB', alpha=0.8)\n",
        "axes[0].bar(x_pos + width/2, impact_df['without_feature_desc'], width, \n",
        "            label='Without Feature', color='#A23B72', alpha=0.8)\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(impact_df['feature'], rotation=45, ha='right')\n",
        "axes[0].set_ylabel('Average # Descendants')\n",
        "axes[0].set_title('Feature Impact: Descendants')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].bar(x_pos - width/2, impact_df['with_feature_dl'], width, \n",
        "            label='With Feature', color='#2E86AB', alpha=0.8)\n",
        "axes[1].bar(x_pos + width/2, impact_df['without_feature_dl'], width, \n",
        "            label='Without Feature', color='#A23B72', alpha=0.8)\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(impact_df['feature'], rotation=45, ha='right')\n",
        "axes[1].set_ylabel('Average Downloads')\n",
        "axes[1].set_title('Feature Impact: Downloads')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/fitness_feature_impact.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary: Evolutionary Landscape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ARCHITECTURAL FITNESS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"   Models analyzed: {len(df_fit):,}\")\n",
        "print(f\"   Total descendants tracked: {df_fit['num_descendants'].sum():,}\")\n",
        "print(f\"   Total downloads: {df_fit['downloads'].sum():,}\")\n",
        "\n",
        "print(f\"\\nüèÜ Most Fit by Size:\")\n",
        "best_size = fitness_by_size[('num_descendants', 'mean')].idxmax()\n",
        "print(f\"   Size bin: {best_size}\")\n",
        "print(f\"   Average descendants: {fitness_by_size.loc[best_size, ('num_descendants', 'mean'):.1f}\")\n",
        "\n",
        "print(f\"\\nüëë Most Fit Family:\")\n",
        "best_family = family_fitness[('num_descendants', 'mean')].idxmax()\n",
        "print(f\"   Family: {best_family.title()}\")\n",
        "print(f\"   Average descendants: {family_fitness.loc[best_family, ('num_descendants', 'mean'):.1f}\")\n",
        "\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(f\"   ‚Ä¢ Models with GQA: {impact_df[impact_df['feature']=='gqa']['with_feature_desc'].values[0]:.1f} avg descendants\")\n",
        "print(f\"   ‚Ä¢ Models without GQA: {impact_df[impact_df['feature']=='gqa']['without_feature_desc'].values[0]:.1f} avg descendants\")\n",
        "if 'moe' in impact_df['feature'].values:\n",
        "    print(f\"   ‚Ä¢ Models with MoE: {impact_df[impact_df['feature']=='moe']['with_feature_desc'].values[0]:.1f} avg descendants\")\n",
        "    print(f\"   ‚Ä¢ Models without MoE: {impact_df[impact_df['feature']=='moe']['without_feature_desc'].values[0]:.1f} avg descendants\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
