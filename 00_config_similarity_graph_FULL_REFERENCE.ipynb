{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config Similarity Graph Analysis\n",
        "\n",
        "**Goal**: Replicate AI Ecosystem paper analyses using config-based similarity and architectural drift, parallel to the trait-based analyses in the original paper.\n",
        "\n",
        "**Key Questions**:\n",
        "1. Which models are architecturally similar?\n",
        "2. How does architecture drift along parent-child edges?\n",
        "3. Which config fields mutate most frequently?\n",
        "4. How does config similarity relate to family trees?\n",
        "5. What is the mutational landscape of config changes?\n",
        "6. How does config drift correlate with behavioral/capability changes?\n",
        "\n",
        "**Methodology**:\n",
        "- **Gower distance** for mixed numeric + categorical similarity (primary metric)\n",
        "- **L2, L1, cosine similarity** for comparison\n",
        "- **Config drift analysis** along parent-child edges\n",
        "- **Mutational landscape** analysis (which fields change most)\n",
        "- **Subgraph analysis** (architectural similarity in descendant clusters)\n",
        "- **Drift by depth** (cumulative drift along lineage paths)\n",
        "- **Behavioral drift** (config change → capability change, when data available)\n",
        "\n",
        "**This notebook implements the full \"Master Plan\" for config-based similarity analysis**, replicating the AI Ecosystem paper methodology but grounded in architectural similarity rather than metadata traits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Style matching main repo\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load config data\n",
        "df = pd.read_csv('data/model_configs_expanded.csv', low_memory=False)\n",
        "print(f\"Loaded {len(df):,} models with config.json\")\n",
        "print(f\"Total columns: {len(df.columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prepare Config Features for Similarity Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define feature groups for similarity computation\n",
        "\n",
        "# Core architecture features (high weight)\n",
        "architecture_features = [\n",
        "    'config_model_type',\n",
        "    'config_hidden_size',\n",
        "    'config_num_hidden_layers',\n",
        "    'config_num_attention_heads',\n",
        "    'config_intermediate_size'\n",
        "]\n",
        "\n",
        "# Capacity features (medium weight)\n",
        "capacity_features = [\n",
        "    'config_vocab_size',\n",
        "    'config_max_position_embeddings',\n",
        "    'config_num_key_value_heads'\n",
        "]\n",
        "\n",
        "# Precision/compute features (lower weight)\n",
        "precision_features = [\n",
        "    'config_torch_dtype',\n",
        "    'config_rope_theta',\n",
        "    'config_rope_scaling_type'\n",
        "]\n",
        "\n",
        "# Boolean flags (low weight)\n",
        "boolean_features = [\n",
        "    'uses_moe',\n",
        "    'uses_gqa',\n",
        "    'uses_rope',\n",
        "    'uses_quantization'\n",
        "]\n",
        "\n",
        "# All features for similarity\n",
        "all_features = architecture_features + capacity_features + precision_features + boolean_features\n",
        "\n",
        "# Filter to features that exist in dataframe\n",
        "available_features = [f for f in all_features if f in df.columns]\n",
        "print(f\"Available features for similarity: {len(available_features)}\")\n",
        "print(f\"Features: {available_features[:10]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implement Gower Distance (Recommended for Mixed Data Types)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gower_distance(x, y, numeric_cols, categorical_cols, boolean_cols):\n",
        "    \"\"\"\n",
        "    Compute Gower distance between two config vectors.\n",
        "    \n",
        "    For numeric: normalized absolute difference\n",
        "    For categorical: 0 if same, 1 if different\n",
        "    For boolean: 0 if same, 1 if different\n",
        "    Missing values: ignored in that dimension\n",
        "    \"\"\"\n",
        "    distance = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    # Numeric features: normalized absolute difference\n",
        "    for col in numeric_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val = x[col]\n",
        "            y_val = y[col]\n",
        "            \n",
        "            # Skip if either is missing\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            \n",
        "            # Convert to numeric\n",
        "            try:\n",
        "                x_num = float(x_val)\n",
        "                y_num = float(y_val)\n",
        "                \n",
        "                # Normalized difference (using max-min normalization)\n",
        "                # For now, use absolute difference normalized by max value\n",
        "                max_val = max(abs(x_num), abs(y_num))\n",
        "                if max_val > 0:\n",
        "                    distance += abs(x_num - y_num) / max_val\n",
        "                else:\n",
        "                    distance += 0  # Both are 0\n",
        "                count += 1\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "    \n",
        "    # Categorical features: 0 if same, 1 if different\n",
        "    for col in categorical_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val = x[col]\n",
        "            y_val = y[col]\n",
        "            \n",
        "            # Skip if either is missing\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            \n",
        "            # Compare as strings\n",
        "            if str(x_val) != str(y_val):\n",
        "                distance += 1.0\n",
        "            count += 1\n",
        "    \n",
        "    # Boolean features: 0 if same, 1 if different\n",
        "    for col in boolean_cols:\n",
        "        if col in x.index and col in y.index:\n",
        "            x_val = x[col]\n",
        "            y_val = y[col]\n",
        "            \n",
        "            # Skip if either is missing\n",
        "            if pd.isna(x_val) or pd.isna(y_val):\n",
        "                continue\n",
        "            \n",
        "            # Normalize boolean values\n",
        "            x_bool = bool(x_val) if not pd.isna(x_val) else False\n",
        "            y_bool = bool(y_val) if not pd.isna(y_val) else False\n",
        "            \n",
        "            if x_bool != y_bool:\n",
        "                distance += 1.0\n",
        "            count += 1\n",
        "    \n",
        "    # Return average distance (0 = identical, 1 = completely different)\n",
        "    if count > 0:\n",
        "        return distance / count\n",
        "    else:\n",
        "        return 1.0  # No common features = maximum distance\n",
        "\n",
        "print(\"Gower distance function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Categorize Features by Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Categorize features by data type\n",
        "numeric_features = []\n",
        "categorical_features = []\n",
        "boolean_feature_list = []\n",
        "\n",
        "for feat in available_features:\n",
        "    if feat in df.columns:\n",
        "        # Check data type\n",
        "        sample_values = df[feat].dropna().head(100)\n",
        "        \n",
        "        if len(sample_values) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Try to convert to numeric\n",
        "        try:\n",
        "            pd.to_numeric(sample_values, errors='raise')\n",
        "            numeric_features.append(feat)\n",
        "        except (ValueError, TypeError):\n",
        "            # Check if boolean-like\n",
        "            unique_vals = sample_values.unique()\n",
        "            if len(unique_vals) <= 2 and set(str(v).lower() for v in unique_vals).issubset({'true', 'false', '1', '0', 'yes', 'no', 'nan'}):\n",
        "                boolean_feature_list.append(feat)\n",
        "            else:\n",
        "                categorical_features.append(feat)\n",
        "\n",
        "print(f\"Numeric features: {len(numeric_features)}\")\n",
        "print(f\"  {numeric_features[:5]}...\")\n",
        "print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
        "print(f\"  {categorical_features[:5]}...\")\n",
        "print(f\"\\nBoolean features: {len(boolean_feature_list)}\")\n",
        "print(f\"  {boolean_feature_list}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Similarity Matrix (Sample for MVP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For MVP, sample a subset of models to make computation feasible\n",
        "# Focus on models with complete config data\n",
        "\n",
        "# Filter to models with at least some key features\n",
        "key_features = ['config_hidden_size', 'config_num_hidden_layers', 'config_model_type']\n",
        "df_complete = df[df[key_features].notna().any(axis=1)].copy()\n",
        "\n",
        "# Sample for MVP (can increase later)\n",
        "SAMPLE_SIZE = 1000  # Start with 1000 models for MVP\n",
        "if len(df_complete) > SAMPLE_SIZE:\n",
        "    # Stratified sample by family if available\n",
        "    if 'family' in df_complete.columns:\n",
        "        df_sample = df_complete.groupby('family', group_keys=False).apply(\n",
        "            lambda x: x.sample(min(len(x), SAMPLE_SIZE // len(df_complete['family'].unique()) + 1))\n",
        "        ).head(SAMPLE_SIZE)\n",
        "    else:\n",
        "        df_sample = df_complete.sample(n=SAMPLE_SIZE, random_state=42)\n",
        "else:\n",
        "    df_sample = df_complete.copy()\n",
        "\n",
        "print(f\"Sampling {len(df_sample):,} models for similarity computation\")\n",
        "print(f\"This will compute {len(df_sample) * (len(df_sample) - 1) // 2:,} pairwise distances\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Architecture Phylogeny: Config Drift Along Parent-Child Edges\n",
        "\n",
        "**Goal**: Measure how architecture drifts along family tree edges, replicating the trait drift analysis from the AI Ecosystem paper.\n",
        "\n",
        "**Key Questions**:\n",
        "- Do fine-tunes preserve architecture?\n",
        "- Which families mutate architecture the most?\n",
        "- What is the distribution of config drift within vs between families?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load family graph to analyze parent-child relationships\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Try to load the family graph\n",
        "G_family = None\n",
        "try:\n",
        "    graph_paths = [\n",
        "        'data/ai_ecosystem_graph_finetune_fulljson.pkl',\n",
        "        'data/ai_ecosystem_graph_nomerges.pkl',\n",
        "        'data/ai_ecosystem_graph.pkl'\n",
        "    ]\n",
        "    for path in graph_paths:\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'rb') as f:\n",
        "                G_family = pickle.load(f)\n",
        "            print(f\"Loaded family graph from {path}\")\n",
        "            print(f\"  Nodes: {len(G_family.nodes):,}\")\n",
        "            print(f\"  Edges: {len(G_family.edges):,}\")\n",
        "            break\n",
        "except Exception as e:\n",
        "    print(f\"Could not load graph: {e}\")\n",
        "    print(\"Will compute drift from parent_model columns in dataframe\")\n",
        "\n",
        "# Compute config drift for parent-child pairs\n",
        "if G_family is not None:\n",
        "    # Extract parent-child pairs from graph\n",
        "    parent_child_pairs = []\n",
        "    for parent, child in G_family.edges():\n",
        "        if parent in df['modelId'].values and child in df['modelId'].values:\n",
        "            parent_child_pairs.append((parent, child))\n",
        "    \n",
        "    print(f\"Found {len(parent_child_pairs):,} parent-child pairs in graph\")\n",
        "else:\n",
        "    # Fallback: use parent_model columns from dataframe\n",
        "    print(\"Using parent_model columns from dataframe\")\n",
        "    parent_child_pairs = []\n",
        "    \n",
        "    # Check for parent columns\n",
        "    parent_cols = ['parent_model', 'finetune_parent', 'quantized_parent', 'adapter_parent', 'merge_parent']\n",
        "    available_parent_cols = [col for col in parent_cols if col in df.columns]\n",
        "    \n",
        "    if len(available_parent_cols) > 0:\n",
        "        for idx, row in df.iterrows():\n",
        "            model_id = row['modelId']\n",
        "            for col in available_parent_cols:\n",
        "                if pd.notna(row[col]):\n",
        "                    try:\n",
        "                        parents = eval(row[col]) if isinstance(row[col], str) else row[col]\n",
        "                        if isinstance(parents, list):\n",
        "                            for parent in parents:\n",
        "                                if parent in df['modelId'].values:\n",
        "                                    parent_child_pairs.append((parent, model_id))\n",
        "                    except:\n",
        "                        continue\n",
        "        \n",
        "        print(f\"Found {len(parent_child_pairs):,} parent-child pairs from dataframe columns\")\n",
        "\n",
        "# Sample pairs for analysis (if too many)\n",
        "MAX_PAIRS = 5000\n",
        "if len(parent_child_pairs) > MAX_PAIRS:\n",
        "    import random\n",
        "    parent_child_pairs = random.sample(parent_child_pairs, MAX_PAIRS)\n",
        "    print(f\"Sampled {len(parent_child_pairs):,} pairs for analysis\")\n",
        "\n",
        "print(f\"\\\\nTotal parent-child pairs to analyze: {len(parent_child_pairs):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute config drift for each parent-child pair\n",
        "print(\"Computing config drift for parent-child pairs...\")\n",
        "\n",
        "drift_data = []\n",
        "for i, (parent_id, child_id) in enumerate(parent_child_pairs):\n",
        "    if i % 500 == 0 and i > 0:\n",
        "        print(f\"  Processed {i}/{len(parent_child_pairs)} pairs...\")\n",
        "    \n",
        "    # Get config vectors\n",
        "    parent_row = df[df['modelId'] == parent_id]\n",
        "    child_row = df[df['modelId'] == child_id]\n",
        "    \n",
        "    if len(parent_row) == 0 or len(child_row) == 0:\n",
        "        continue\n",
        "    \n",
        "    parent_vec = parent_row.iloc[0][available_features]\n",
        "    child_vec = child_row.iloc[0][available_features]\n",
        "    \n",
        "    # Compute Gower distance (drift)\n",
        "    drift = gower_distance(\n",
        "        parent_vec,\n",
        "        child_vec,\n",
        "        numeric_features,\n",
        "        categorical_features,\n",
        "        boolean_feature_list\n",
        "    )\n",
        "    \n",
        "    # Get family info if available\n",
        "    parent_family = parent_row.iloc[0].get('family', 'Unknown')\n",
        "    child_family = child_row.iloc[0].get('family', 'Unknown')\n",
        "    same_family = parent_family == child_family and parent_family != 'Unknown'\n",
        "    \n",
        "    drift_data.append({\n",
        "        'parent_id': parent_id,\n",
        "        'child_id': child_id,\n",
        "        'drift': drift,\n",
        "        'similarity': 1 - drift,\n",
        "        'parent_family': parent_family,\n",
        "        'child_family': child_family,\n",
        "        'same_family': same_family\n",
        "    })\n",
        "\n",
        "df_drift = pd.DataFrame(drift_data)\n",
        "print(f\"\\\\n✓ Computed drift for {len(df_drift):,} parent-child pairs\")\n",
        "print(f\"  Mean drift: {df_drift['drift'].mean():.3f}\")\n",
        "print(f\"  Median drift: {df_drift['drift'].median():.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize config drift distributions\n",
        "if len(df_drift) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Overall drift distribution\n",
        "    axes[0,0].hist(df_drift['drift'], bins=50, color='steelblue', alpha=0.7, edgecolor='white')\n",
        "    axes[0,0].axvline(df_drift['drift'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: {df_drift[\\\"drift\\\"].median():.3f}')\n",
        "    axes[0,0].set_xlabel('Config Drift (Gower Distance)', fontsize=11)\n",
        "    axes[0,0].set_ylabel('Count', fontsize=11)\n",
        "    axes[0,0].set_title('Distribution of Config Drift Along Parent-Child Edges', fontsize=13)\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 2. Within-family vs between-family drift\n",
        "    if df_drift['same_family'].sum() > 0:\n",
        "        within_family = df_drift[df_drift['same_family'] == True]['drift']\n",
        "        between_family = df_drift[df_drift['same_family'] == False]['drift']\n",
        "        \n",
        "        axes[0,1].hist([within_family, between_family], bins=30, label=['Within Family', 'Between Families'], \n",
        "                      alpha=0.7, color=['seagreen', 'coral'], edgecolor='white')\n",
        "        axes[0,1].set_xlabel('Config Drift', fontsize=11)\n",
        "        axes[0,1].set_ylabel('Count', fontsize=11)\n",
        "        axes[0,1].set_title('Config Drift: Within vs Between Families', fontsize=13)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        print(f\"\\\\nWithin-family drift: mean={within_family.mean():.3f}, median={within_family.median():.3f}\")\n",
        "        print(f\"Between-family drift: mean={between_family.mean():.3f}, median={between_family.median():.3f}\")\n",
        "    \n",
        "    # 3. Drift by family (top families)\n",
        "    if 'parent_family' in df_drift.columns:\n",
        "        family_drift = df_drift.groupby('parent_family')['drift'].agg(['mean', 'median', 'count']).sort_values('count', ascending=False)\n",
        "        top_families = family_drift.head(10)\n",
        "        \n",
        "        x_pos = np.arange(len(top_families))\n",
        "        width = 0.35\n",
        "        axes[1,0].bar(x_pos - width/2, top_families['mean'], width, label='Mean', color='steelblue', alpha=0.7)\n",
        "        axes[1,0].bar(x_pos + width/2, top_families['median'], width, label='Median', color='coral', alpha=0.7)\n",
        "        axes[1,0].set_xticks(x_pos)\n",
        "        axes[1,0].set_xticklabels(top_families.index, rotation=45, ha='right')\n",
        "        axes[1,0].set_ylabel('Config Drift', fontsize=11)\n",
        "        axes[1,0].set_title('Config Drift by Family (Top 10)', fontsize=13)\n",
        "        axes[1,0].legend()\n",
        "        axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 4. Cumulative drift distribution\n",
        "    sorted_drift = np.sort(df_drift['drift'])\n",
        "    cumulative = np.arange(1, len(sorted_drift) + 1) / len(sorted_drift)\n",
        "    axes[1,1].plot(sorted_drift, cumulative, linewidth=2, color='purple')\n",
        "    axes[1,1].axvline(df_drift['drift'].median(), color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
        "    axes[1,1].set_xlabel('Config Drift', fontsize=11)\n",
        "    axes[1,1].set_ylabel('Cumulative Fraction', fontsize=11)\n",
        "    axes[1,1].set_title('Cumulative Distribution of Config Drift', fontsize=13)\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('figures/config_drift_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Save drift data\n",
        "    df_drift.to_csv('config_drift_pairs.csv', index=False)\n",
        "    print(\"\\\\n✓ Drift data saved to config_drift_pairs.csv\")\n",
        "else:\n",
        "    print(\"No drift data to visualize\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Mutational Landscape: Which Config Fields Drift Most?\n",
        "\n",
        "**Goal**: Identify which config fields mutate most frequently along parent-child edges, parallel to trait mutation analysis in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze which config fields change most frequently\n",
        "print(\"Analyzing mutational landscape...\")\n",
        "\n",
        "mutation_counts = {feat: {'changed': 0, 'unchanged': 0, 'missing': 0} for feat in available_features}\n",
        "\n",
        "for parent_id, child_id in parent_child_pairs[:min(5000, len(parent_child_pairs))]:\n",
        "    parent_row = df[df['modelId'] == parent_id]\n",
        "    child_row = df[df['modelId'] == child_id]\n",
        "    \n",
        "    if len(parent_row) == 0 or len(child_row) == 0:\n",
        "        continue\n",
        "    \n",
        "    parent_vec = parent_row.iloc[0]\n",
        "    child_vec = child_row.iloc[0]\n",
        "    \n",
        "    for feat in available_features:\n",
        "        parent_val = parent_vec.get(feat)\n",
        "        child_val = child_vec.get(feat)\n",
        "        \n",
        "        # Check if missing\n",
        "        if pd.isna(parent_val) or pd.isna(child_val):\n",
        "            mutation_counts[feat]['missing'] += 1\n",
        "            continue\n",
        "        \n",
        "        # Check if changed\n",
        "        if feat in numeric_features:\n",
        "            try:\n",
        "                parent_num = float(parent_val)\n",
        "                child_num = float(child_val)\n",
        "                if abs(parent_num - child_num) > 1e-6:  # Numeric difference\n",
        "                    mutation_counts[feat]['changed'] += 1\n",
        "                else:\n",
        "                    mutation_counts[feat]['unchanged'] += 1\n",
        "            except:\n",
        "                if str(parent_val) != str(child_val):\n",
        "                    mutation_counts[feat]['changed'] += 1\n",
        "                else:\n",
        "                    mutation_counts[feat]['unchanged'] += 1\n",
        "        else:\n",
        "            if str(parent_val) != str(child_val):\n",
        "                mutation_counts[feat]['changed'] += 1\n",
        "            else:\n",
        "                mutation_counts[feat]['unchanged'] += 1\n",
        "\n",
        "# Compute mutation rates\n",
        "mutation_rates = []\n",
        "for feat, counts in mutation_counts.items():\n",
        "    total = counts['changed'] + counts['unchanged']\n",
        "    if total > 0:\n",
        "        rate = counts['changed'] / total\n",
        "        mutation_rates.append({\n",
        "            'feature': feat,\n",
        "            'mutation_rate': rate,\n",
        "            'n_changed': counts['changed'],\n",
        "            'n_unchanged': counts['unchanged'],\n",
        "            'n_missing': counts['missing'],\n",
        "            'total_pairs': total\n",
        "        })\n",
        "\n",
        "df_mutations = pd.DataFrame(mutation_rates).sort_values('mutation_rate', ascending=False)\n",
        "print(f\"\\\\n✓ Analyzed mutations for {len(df_mutations)} features\")\n",
        "print(f\"\\\\nTop 10 most frequently mutated features:\")\n",
        "print(df_mutations.head(10)[['feature', 'mutation_rate', 'n_changed', 'total_pairs']].to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize mutational landscape\n",
        "if len(df_mutations) > 0:\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Top: Mutation rates for top features\n",
        "    top_mutations = df_mutations.head(20)\n",
        "    colors = ['coral' if rate > 0.5 else 'steelblue' for rate in top_mutations['mutation_rate']]\n",
        "    \n",
        "    bars = axes[0].barh(range(len(top_mutations)), top_mutations['mutation_rate'], color=colors, alpha=0.7)\n",
        "    axes[0].set_yticks(range(len(top_mutations)))\n",
        "    axes[0].set_yticklabels(top_mutations['feature'], fontsize=9)\n",
        "    axes[0].invert_yaxis()\n",
        "    axes[0].set_xlabel('Mutation Rate', fontsize=11)\n",
        "    axes[0].set_title('Top 20 Most Frequently Mutated Config Features', fontsize=13)\n",
        "    axes[0].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (idx, row) in enumerate(top_mutations.iterrows()):\n",
        "        axes[0].text(row['mutation_rate'] + 0.01, i, f\\\"{row['mutation_rate']:.2%}\\\", va='center', fontsize=8)\n",
        "    \n",
        "    # Bottom: Mutation counts by feature type\n",
        "    df_mutations['feature_type'] = df_mutations['feature'].apply(\n",
        "        lambda x: 'Numeric' if x in numeric_features else ('Categorical' if x in categorical_features else 'Boolean')\n",
        "    )\n",
        "    \n",
        "    type_summary = df_mutations.groupby('feature_type').agg({\n",
        "        'mutation_rate': 'mean',\n",
        "        'n_changed': 'sum',\n",
        "        'total_pairs': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    x_pos = np.arange(len(type_summary))\n",
        "    width = 0.35\n",
        "    axes[1].bar(x_pos - width/2, type_summary['mutation_rate'], width, label='Mean Mutation Rate', color='steelblue', alpha=0.7)\n",
        "    axes[1].bar(x_pos + width/2, type_summary['n_changed'] / type_summary['total_pairs'], width, \n",
        "               label='Overall Mutation Rate', color='coral', alpha=0.7)\n",
        "    axes[1].set_xticks(x_pos)\n",
        "    axes[1].set_xticklabels(type_summary['feature_type'])\n",
        "    axes[1].set_ylabel('Mutation Rate', fontsize=11)\n",
        "    axes[1].set_title('Mutation Rates by Feature Type', fontsize=13)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('figures/mutational_landscape.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Save mutation data\n",
        "    df_mutations.to_csv('config_mutation_rates.csv', index=False)\n",
        "    print(\"\\\\n✓ Mutation data saved to config_mutation_rates.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Subgraph Analysis: Architectural Similarity in Descendant Clusters\n",
        "\n",
        "**Goal**: Analyze config similarity within subgraphs (descendant clusters), replicating the subgraph trait correlation analysis from the AI Ecosystem paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze config similarity within subgraphs (descendant clusters)\n",
        "if G_family is not None:\n",
        "    print(\"Analyzing config similarity within descendant subgraphs...\")\n",
        "    \n",
        "    # Find root nodes (nodes with no incoming edges)\n",
        "    root_nodes = [n for n in G_family.nodes() if G_family.in_degree(n) == 0]\n",
        "    print(f\"Found {len(root_nodes):,} root nodes\")\n",
        "    \n",
        "    # Analyze top root nodes by number of descendants\n",
        "    root_descendant_counts = {}\n",
        "    for root in root_nodes[:min(20, len(root_nodes))]:  # Top 20 roots\n",
        "        descendants = set(nx.descendants(G_family, root))\n",
        "        root_descendant_counts[root] = len(descendants)\n",
        "    \n",
        "    # Sort by descendant count\n",
        "    top_roots = sorted(root_descendant_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    \n",
        "    subgraph_stats = []\n",
        "    \n",
        "    for root_id, n_descendants in top_roots:\n",
        "        if n_descendants < 5:  # Skip small subgraphs\n",
        "            continue\n",
        "        \n",
        "        # Get subgraph\n",
        "        descendants = set(nx.descendants(G_family, root_id)) | {root_id}\n",
        "        subgraph_nodes = [n for n in descendants if n in df['modelId'].values]\n",
        "        \n",
        "        if len(subgraph_nodes) < 5:\n",
        "            continue\n",
        "        \n",
        "        # Get config vectors for subgraph\n",
        "        subgraph_df = df[df['modelId'].isin(subgraph_nodes)][available_features].copy()\n",
        "        \n",
        "        if len(subgraph_df) < 2:\n",
        "            continue\n",
        "        \n",
        "        # Compute pairwise similarities within subgraph\n",
        "        subgraph_similarities = []\n",
        "        subgraph_indices = subgraph_df.index.tolist()\n",
        "        \n",
        "        for i in range(len(subgraph_indices)):\n",
        "            for j in range(i+1, len(subgraph_indices)):\n",
        "                vec_i = subgraph_df.loc[subgraph_indices[i]]\n",
        "                vec_j = subgraph_df.loc[subgraph_indices[j]]\n",
        "                \n",
        "                dist = gower_distance(\n",
        "                    vec_i, vec_j,\n",
        "                    numeric_features,\n",
        "                    categorical_features,\n",
        "                    boolean_feature_list\n",
        "                )\n",
        "                subgraph_similarities.append(1 - dist)\n",
        "        \n",
        "        if len(subgraph_similarities) > 0:\n",
        "            mean_sim = np.mean(subgraph_similarities)\n",
        "            median_sim = np.median(subgraph_similarities)\n",
        "            \n",
        "            # Get family info\n",
        "            root_family = df[df['modelId'] == root_id]['family'].iloc[0] if len(df[df['modelId'] == root_id]) > 0 else 'Unknown'\n",
        "            \n",
        "            subgraph_stats.append({\n",
        "                'root_id': root_id,\n",
        "                'n_nodes': len(subgraph_nodes),\n",
        "                'n_descendants': n_descendants,\n",
        "                'mean_similarity': mean_sim,\n",
        "                'median_similarity': median_sim,\n",
        "                'family': root_family\n",
        "            })\n",
        "    \n",
        "    df_subgraph_stats = pd.DataFrame(subgraph_stats)\n",
        "    \n",
        "    if len(df_subgraph_stats) > 0:\n",
        "        print(f\"\\\\n✓ Analyzed {len(df_subgraph_stats)} subgraphs\")\n",
        "        print(f\"\\\\nSubgraph similarity statistics:\")\n",
        "        print(df_subgraph_stats[['root_id', 'n_nodes', 'mean_similarity', 'family']].head(10).to_string(index=False))\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # Scatter: subgraph size vs similarity\n",
        "        axes[0].scatter(df_subgraph_stats['n_nodes'], df_subgraph_stats['mean_similarity'], \n",
        "                       alpha=0.6, s=100, c='steelblue')\n",
        "        axes[0].set_xlabel('Subgraph Size (Number of Nodes)', fontsize=11)\n",
        "        axes[0].set_ylabel('Mean Config Similarity', fontsize=11)\n",
        "        axes[0].set_title('Subgraph Size vs Config Similarity', fontsize=13)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Similarity by family\n",
        "        if 'family' in df_subgraph_stats.columns and df_subgraph_stats['family'].nunique() > 1:\n",
        "            family_similarity = df_subgraph_stats.groupby('family')['mean_similarity'].agg(['mean', 'count']).sort_values('count', ascending=False).head(10)\n",
        "            \n",
        "            x_pos = np.arange(len(family_similarity))\n",
        "            axes[1].bar(x_pos, family_similarity['mean'], color='coral', alpha=0.7)\n",
        "            axes[1].set_xticks(x_pos)\n",
        "            axes[1].set_xticklabels(family_similarity.index, rotation=45, ha='right')\n",
        "            axes[1].set_ylabel('Mean Config Similarity', fontsize=11)\n",
        "            axes[1].set_title('Mean Subgraph Similarity by Family', fontsize=13)\n",
        "            axes[1].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # Add count labels\n",
        "            for i, (idx, row) in enumerate(family_similarity.iterrows()):\n",
        "                axes[1].text(i, row['mean'] + 0.01, f\\\"n={int(row['count'])}\\\", ha='center', fontsize=8)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/subgraph_similarity_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save subgraph stats\n",
        "        df_subgraph_stats.to_csv('subgraph_similarity_stats.csv', index=False)\n",
        "        print(\"\\\\n✓ Subgraph stats saved to subgraph_similarity_stats.csv\")\n",
        "    else:\n",
        "        print(\"No subgraph statistics computed\")\n",
        "else:\n",
        "    print(\"Family graph not available - skipping subgraph analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Drift by Depth: Config Drift Curves\n",
        "\n",
        "**Goal**: Measure cumulative config drift along lineage paths (root → ... → leaf), parallel to trait drift curves in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze config drift by depth in lineage trees\n",
        "if G_family is not None:\n",
        "    print(\"Analyzing config drift by depth...\")\n",
        "    \n",
        "    # Find some deep lineages\n",
        "    def get_lineage_paths(root, max_depth=10, max_paths=100):\n",
        "        \\\"\\\"\\\"Get paths from root to leaves\\\"\\\"\\\"\n",
        "        paths = []\n",
        "        leaves = [n for n in G_family.nodes() if G_family.out_degree(n) == 0 and nx.has_path(G_family, root, n)]\n",
        "        \n",
        "        for leaf in leaves[:max_paths]:\n",
        "            try:\n",
        "                path = nx.shortest_path(G_family, root, leaf)\n",
        "                if len(path) <= max_depth:\n",
        "                    paths.append(path)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        return paths\n",
        "    \n",
        "    # Analyze drift along paths for top roots\n",
        "    depth_drift_data = []\n",
        "    \n",
        "    for root_id, _ in top_roots[:5]:  # Top 5 roots\n",
        "        if root_id not in df['modelId'].values:\n",
        "            continue\n",
        "        \n",
        "        paths = get_lineage_paths(root_id, max_depth=8, max_paths=50)\n",
        "        \n",
        "        for path in paths:\n",
        "            # Filter to nodes with config data\n",
        "            path_with_config = [n for n in path if n in df['modelId'].values]\n",
        "            \n",
        "            if len(path_with_config) < 2:\n",
        "                continue\n",
        "            \n",
        "            # Get root config\n",
        "            root_config = df[df['modelId'] == path_with_config[0]][available_features].iloc[0]\n",
        "            \n",
        "            # Compute cumulative drift along path\n",
        "            for i, node_id in enumerate(path_with_config[1:], 1):\n",
        "                node_config = df[df['modelId'] == node_id][available_features].iloc[0]\n",
        "                \n",
        "                drift = gower_distance(\n",
        "                    root_config, node_config,\n",
        "                    numeric_features,\n",
        "                    categorical_features,\n",
        "                    boolean_feature_list\n",
        "                )\n",
        "                \n",
        "                depth_drift_data.append({\n",
        "                    'root_id': root_id,\n",
        "                    'node_id': node_id,\n",
        "                    'depth': i,\n",
        "                    'cumulative_drift': drift,\n",
        "                    'path_length': len(path_with_config)\n",
        "                })\n",
        "    \n",
        "    df_depth_drift = pd.DataFrame(depth_drift_data)\n",
        "    \n",
        "    if len(df_depth_drift) > 0:\n",
        "        print(f\"\\\\n✓ Analyzed drift along {df_depth_drift['root_id'].nunique()} lineage trees\")\n",
        "        \n",
        "        # Visualize drift by depth\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # Average drift by depth\n",
        "        depth_stats = df_depth_drift.groupby('depth')['cumulative_drift'].agg(['mean', 'median', 'std', 'count']).reset_index()\n",
        "        \n",
        "        axes[0].plot(depth_stats['depth'], depth_stats['mean'], marker='o', linewidth=2, label='Mean', color='steelblue')\n",
        "        axes[0].fill_between(depth_stats['depth'], \n",
        "                            depth_stats['mean'] - depth_stats['std'],\n",
        "                            depth_stats['mean'] + depth_stats['std'],\n",
        "                            alpha=0.2, color='steelblue')\n",
        "        axes[0].plot(depth_stats['depth'], depth_stats['median'], marker='s', linewidth=2, label='Median', color='coral', linestyle='--')\n",
        "        axes[0].set_xlabel('Depth in Lineage Tree', fontsize=11)\n",
        "        axes[0].set_ylabel('Cumulative Config Drift', fontsize=11)\n",
        "        axes[0].set_title('Config Drift vs Depth in Lineage Trees', fontsize=13)\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Distribution of drift at different depths\n",
        "        depth_samples = [1, 3, 5, 7]\n",
        "        available_depths = [d for d in depth_samples if d in df_depth_drift['depth'].values]\n",
        "        \n",
        "        if len(available_depths) > 0:\n",
        "            drift_by_depth = [df_depth_drift[df_depth_drift['depth'] == d]['cumulative_drift'].values for d in available_depths]\n",
        "            axes[1].boxplot(drift_by_depth, labels=[f'Depth {d}' for d in available_depths])\n",
        "            axes[1].set_xlabel('Depth', fontsize=11)\n",
        "            axes[1].set_ylabel('Cumulative Config Drift', fontsize=11)\n",
        "            axes[1].set_title('Distribution of Config Drift at Different Depths', fontsize=13)\n",
        "            axes[1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/drift_by_depth.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save depth drift data\n",
        "        df_depth_drift.to_csv('config_drift_by_depth.csv', index=False)\n",
        "        print(\"\\\\n✓ Depth drift data saved to config_drift_by_depth.csv\")\n",
        "    else:\n",
        "        print(\"No depth drift data computed\")\n",
        "else:\n",
        "    print(\"Family graph not available - skipping depth analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Behavioral Drift: Config Change → Capability Change\n",
        "\n",
        "**Goal**: Analyze how config changes correlate with behavioral/capability changes (when behavioral data is available).\n",
        "\n",
        "**Note**: This section requires joining with behavioral datasets (LMArena, Arena-Hard-Auto, P2L). Placeholder code is provided for when such data becomes available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Placeholder for behavioral drift analysis\n",
        "# This requires joining config data with behavioral evaluation datasets\n",
        "\n",
        "print(\"Behavioral drift analysis (placeholder)\")\n",
        "print(\"\\\\nThis analysis requires:\")\n",
        "print(\"  1. LMArena Elo scores or Arena-Hard-Auto scores\")\n",
        "print(\"  2. P2L (Prompt-to-Label) performance vectors\")\n",
        "print(\"  3. Joining behavioral data with config drift data\")\n",
        "print(\"\\\\nWhen behavioral data is available, this section will:\")\n",
        "print(\"  - Compute Δ_behavior = Elo(child) - Elo(parent)\")\n",
        "print(\"  - Regress capability change on config drift\")\n",
        "print(\"  - Identify which config changes correlate with capability changes\")\n",
        "print(\"  - Analyze prompt-specific behavioral drift (P2L clusters)\")\n",
        "\n",
        "# Example structure for when data is available:\n",
        "# behavioral_data = {\n",
        "#     'modelId': [...],\n",
        "#     'elo_score': [...],\n",
        "#     'arena_hard_score': [...],\n",
        "#     'p2l_vector': [...]\n",
        "# }\n",
        "# \n",
        "# df_behavioral = pd.DataFrame(behavioral_data)\n",
        "# df_drift_with_behavior = df_drift.merge(df_behavioral, left_on='child_id', right_on='modelId', how='inner')\n",
        "# \n",
        "# # Compute behavioral change\n",
        "# df_drift_with_behavior = df_drift_with_behavior.merge(\n",
        "#     df_behavioral, left_on='parent_id', right_on='modelId', \n",
        "#     suffixes=('_child', '_parent'), how='inner'\n",
        "# )\n",
        "# df_drift_with_behavior['delta_elo'] = df_drift_with_behavior['elo_score_child'] - df_drift_with_behavior['elo_score_parent']\n",
        "# \n",
        "# # Regression: capability change ~ config drift\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# X = df_drift_with_behavior[['drift']].values\n",
        "# y = df_drift_with_behavior['delta_elo'].values\n",
        "# model = LinearRegression().fit(X, y)\n",
        "# \n",
        "# print(f\\\"Regression: Δ_Elo ~ Config_Drift\\\")\n",
        "# print(f\\\"  Coefficient: {model.coef_[0]:.3f}\\\")\n",
        "# print(f\\\"  R²: {model.score(X, y):.3f}\\\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map LMArena model names to HuggingFace model IDs\n",
        "# This is a critical step - Arena uses display names, HF uses repo paths\n",
        "\n",
        "if lmarena_data is not None:\n",
        "    print(\"Mapping LMArena model names to HuggingFace modelIds...\")\n",
        "    \n",
        "    # Common mapping patterns\n",
        "    def normalize_model_name(name):\n",
        "        \"\"\"Normalize model name for matching\"\"\"\n",
        "        if pd.isna(name):\n",
        "            return None\n",
        "        name = str(name).lower().strip()\n",
        "        # Remove common prefixes/suffixes\n",
        "        name = name.replace(' (chat)', '').replace(' (instruct)', '').replace(' (base)', '')\n",
        "        name = name.replace('chat-', '').replace('instruct-', '').replace('base-', '')\n",
        "        return name\n",
        "    \n",
        "    # Normalize Arena names\n",
        "    lmarena_data['normalized_name'] = lmarena_data['model'].apply(normalize_model_name) if 'model' in lmarena_data.columns else None\n",
        "    \n",
        "    # Normalize HF modelIds\n",
        "    df['normalized_id'] = df['modelId'].apply(lambda x: normalize_model_name(x.split('/')[-1] if '/' in str(x) else str(x)))\n",
        "    \n",
        "    # Try direct matches first\n",
        "    if 'normalized_name' in lmarena_data.columns:\n",
        "        # Merge on normalized names\n",
        "        df_with_elo = df.merge(\n",
        "            lmarena_data[['model', 'elo_rating', 'elo_uncertainty']].rename(columns={'model': 'arena_model'}),\n",
        "            left_on='normalized_id',\n",
        "            right_on=lmarena_data['normalized_name'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Also try matching on modelId directly\n",
        "        direct_matches = df.merge(\n",
        "            lmarena_data[['model', 'elo_rating', 'elo_uncertainty']].rename(columns={'model': 'arena_model'}),\n",
        "            left_on='modelId',\n",
        "            right_on='model',\n",
        "            how='left',\n",
        "            suffixes=('', '_direct')\n",
        "        )\n",
        "        \n",
        "        # Combine matches\n",
        "        df['elo_rating'] = df_with_elo['elo_rating'].fillna(direct_matches.get('elo_rating', None))\n",
        "        df['elo_uncertainty'] = df_with_elo['elo_uncertainty'].fillna(direct_matches.get('elo_uncertainty', None))\n",
        "        df['arena_model'] = df_with_elo['arena_model'].fillna(direct_matches.get('arena_model', None))\n",
        "    \n",
        "    models_with_elo = df['elo_rating'].notna().sum()\n",
        "    print(f\"✓ Mapped {models_with_elo:,} models with Elo scores\")\n",
        "    print(f\"  Coverage: {models_with_elo/len(df)*100:.1f}% of config dataset\")\n",
        "    \n",
        "    if models_with_elo > 0:\n",
        "        print(f\"\\\\nElo score statistics:\")\n",
        "        print(f\"  Mean: {df['elo_rating'].mean():.1f}\")\n",
        "        print(f\"  Median: {df['elo_rating'].median():.1f}\")\n",
        "        print(f\"  Range: [{df['elo_rating'].min():.1f}, {df['elo_rating'].max():.1f}]\")\n",
        "else:\n",
        "    print(\"Skipping mapping - no LMArena data available\")\n",
        "    df['elo_rating'] = None\n",
        "    df['elo_uncertainty'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Behavioral Drift: Config Change → Capability Change\n",
        "\n",
        "**Goal**: Analyze how config changes correlate with behavioral/capability changes (Elo scores from LMArena).\n",
        "\n",
        "**Key Questions**:\n",
        "- Does config drift predict behavioral drift?\n",
        "- Which architectural changes correlate with capability improvements?\n",
        "- Are some families more behaviorally coherent than others?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute behavioral drift for parent-child pairs with Elo data\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 0:\n",
        "    print(\"Computing behavioral drift (ΔElo) for parent-child pairs...\")\n",
        "    \n",
        "    # Merge Elo scores into drift dataframe\n",
        "    df_drift_with_behavior = df_drift.merge(\n",
        "        df[['modelId', 'elo_rating', 'elo_uncertainty']].rename(columns={\n",
        "            'modelId': 'child_id',\n",
        "            'elo_rating': 'child_elo',\n",
        "            'elo_uncertainty': 'child_elo_uncertainty'\n",
        "        }),\n",
        "        on='child_id',\n",
        "        how='left'\n",
        "    ).merge(\n",
        "        df[['modelId', 'elo_rating', 'elo_uncertainty']].rename(columns={\n",
        "            'modelId': 'parent_id',\n",
        "            'elo_rating': 'parent_elo',\n",
        "            'elo_uncertainty': 'parent_elo_uncertainty'\n",
        "        }),\n",
        "        on='parent_id',\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Compute behavioral drift (ΔElo)\n",
        "    df_drift_with_behavior['delta_elo'] = (\n",
        "        df_drift_with_behavior['child_elo'] - df_drift_with_behavior['parent_elo']\n",
        "    )\n",
        "    \n",
        "    # Filter to pairs where both have Elo scores\n",
        "    df_behavioral_drift = df_drift_with_behavior[\n",
        "        df_drift_with_behavior['child_elo'].notna() & \n",
        "        df_drift_with_behavior['parent_elo'].notna()\n",
        "    ].copy()\n",
        "    \n",
        "    print(f\"\\\\n✓ Found {len(df_behavioral_drift):,} parent-child pairs with behavioral data\")\n",
        "    print(f\"  Mean ΔElo: {df_behavioral_drift['delta_elo'].mean():.2f}\")\n",
        "    print(f\"  Median ΔElo: {df_behavioral_drift['delta_elo'].median():.2f}\")\n",
        "    print(f\"  Improving pairs: {(df_behavioral_drift['delta_elo'] > 0).sum():,} ({(df_behavioral_drift['delta_elo'] > 0).mean()*100:.1f}%)\")\n",
        "    print(f\"  Regressing pairs: {(df_behavioral_drift['delta_elo'] < 0).sum():,} ({(df_behavioral_drift['delta_elo'] < 0).mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Visualize config drift vs behavioral drift\n",
        "    if len(df_behavioral_drift) > 10:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Scatter: Config drift vs Behavioral drift (THE CORE PLOT)\n",
        "        axes[0,0].scatter(df_behavioral_drift['drift'], df_behavioral_drift['delta_elo'], \n",
        "                         alpha=0.5, s=30, c='steelblue', edgecolors='black', linewidth=0.5)\n",
        "        axes[0,0].axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
        "        axes[0,0].axvline(df_behavioral_drift['drift'].median(), color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
        "        axes[0,0].set_xlabel('Config Drift (Gower Distance)', fontsize=11)\n",
        "        axes[0,0].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[0,0].set_title('Config Drift vs Behavioral Drift (Genotype → Phenotype)', fontsize=13)\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add correlation coefficient\n",
        "        from scipy.stats import pearsonr\n",
        "        corr, p_val = pearsonr(df_behavioral_drift['drift'], df_behavioral_drift['delta_elo'])\n",
        "        axes[0,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                      transform=axes[0,0].transAxes, fontsize=10,\n",
        "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 2. Distribution of behavioral drift\n",
        "        axes[0,1].hist(df_behavioral_drift['delta_elo'], bins=50, color='coral', alpha=0.7, edgecolor='white')\n",
        "        axes[0,1].axvline(0, color='red', linestyle='--', linewidth=2, label='No change')\n",
        "        axes[0,1].axvline(df_behavioral_drift['delta_elo'].median(), color='blue', linestyle='--', linewidth=2, \n",
        "                         label=f'Median: {df_behavioral_drift[\\\"delta_elo\\\"].median():.1f}')\n",
        "        axes[0,1].set_xlabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[0,1].set_ylabel('Count', fontsize=11)\n",
        "        axes[0,1].set_title('Distribution of Behavioral Drift', fontsize=13)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # 3. Behavioral drift by family\n",
        "        if 'parent_family' in df_behavioral_drift.columns:\n",
        "            family_behavioral_drift = df_behavioral_drift.groupby('parent_family')['delta_elo'].agg(['mean', 'median', 'count']).sort_values('count', ascending=False).head(10)\n",
        "            \n",
        "            x_pos = np.arange(len(family_behavioral_drift))\n",
        "            width = 0.35\n",
        "            axes[1,0].bar(x_pos - width/2, family_behavioral_drift['mean'], width, label='Mean', color='steelblue', alpha=0.7)\n",
        "            axes[1,0].bar(x_pos + width/2, family_behavioral_drift['median'], width, label='Median', color='coral', alpha=0.7)\n",
        "            axes[1,0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "            axes[1,0].set_xticks(x_pos)\n",
        "            axes[1,0].set_xticklabels(family_behavioral_drift.index, rotation=45, ha='right')\n",
        "            axes[1,0].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "            axes[1,0].set_title('Behavioral Drift by Family (Top 10)', fontsize=13)\n",
        "            axes[1,0].legend()\n",
        "            axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # 4. Quadrant plot: Config drift vs Behavioral drift\n",
        "        median_config_drift = df_behavioral_drift['drift'].median()\n",
        "        median_behavioral_drift = df_behavioral_drift['delta_elo'].median()\n",
        "        \n",
        "        quadrants = {\n",
        "            'High Config, High Behavior': (df_behavioral_drift['drift'] > median_config_drift) & (df_behavioral_drift['delta_elo'] > median_behavioral_drift),\n",
        "            'High Config, Low Behavior': (df_behavioral_drift['drift'] > median_config_drift) & (df_behavioral_drift['delta_elo'] <= median_behavioral_drift),\n",
        "            'Low Config, High Behavior': (df_behavioral_drift['drift'] <= median_config_drift) & (df_behavioral_drift['delta_elo'] > median_behavioral_drift),\n",
        "            'Low Config, Low Behavior': (df_behavioral_drift['drift'] <= median_config_drift) & (df_behavioral_drift['delta_elo'] <= median_behavioral_drift)\n",
        "        }\n",
        "        \n",
        "        colors = {'High Config, High Behavior': 'green', 'High Config, Low Behavior': 'orange',\n",
        "                 'Low Config, High Behavior': 'blue', 'Low Config, Low Behavior': 'red'}\n",
        "        \n",
        "        for quad_name, mask in quadrants.items():\n",
        "            if mask.sum() > 0:\n",
        "                axes[1,1].scatter(df_behavioral_drift[mask]['drift'], df_behavioral_drift[mask]['delta_elo'],\n",
        "                                 alpha=0.5, s=30, label=quad_name, c=colors[quad_name], edgecolors='black', linewidth=0.5)\n",
        "        \n",
        "        axes[1,1].axhline(median_behavioral_drift, color='gray', linestyle='--', alpha=0.5)\n",
        "        axes[1,1].axvline(median_config_drift, color='gray', linestyle='--', alpha=0.5)\n",
        "        axes[1,1].set_xlabel('Config Drift', fontsize=11)\n",
        "        axes[1,1].set_ylabel('Behavioral Drift (ΔElo)', fontsize=11)\n",
        "        axes[1,1].set_title('Config vs Behavioral Drift Quadrants', fontsize=13)\n",
        "        axes[1,1].legend(fontsize=8)\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/config_vs_behavioral_drift.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save behavioral drift data\n",
        "        df_behavioral_drift.to_csv('config_behavioral_drift.csv', index=False)\n",
        "        print(\"\\\\n✓ Behavioral drift data saved to config_behavioral_drift.csv\")\n",
        "    else:\n",
        "        print(\"Not enough behavioral data for visualization\")\n",
        "else:\n",
        "    print(\"No behavioral data available - skipping behavioral drift analysis\")\n",
        "    df_behavioral_drift = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Architecture-Capability Regression\n",
        "\n",
        "**Goal**: Predict behavioral capability (Elo) from architectural features (config.json parameters).\n",
        "\n",
        "**Key Questions**:\n",
        "- Which architectural features are most predictive of capability?\n",
        "- Are there nonlinear effects or diminishing returns?\n",
        "- Do architecture clusters correspond to capability clusters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict Elo from config features\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 50:\n",
        "    print(\"Training models to predict Elo from config features...\")\n",
        "    \n",
        "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import r2_score, mean_squared_error\n",
        "    \n",
        "    # Prepare data\n",
        "    df_elo = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    # Prepare features (numeric only for regression)\n",
        "    feature_cols = numeric_features + boolean_feature_list\n",
        "    X_cols = [col for col in feature_cols if col in df_elo.columns]\n",
        "    \n",
        "    # Fill missing values\n",
        "    X = df_elo[X_cols].fillna(0)\n",
        "    y = df_elo['elo_rating'].values\n",
        "    \n",
        "    # Remove columns with no variance\n",
        "    X = X.loc[:, X.std() > 0]\n",
        "    \n",
        "    print(f\"\\\\nTraining on {len(X):,} models with {len(X.columns)} features\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train models\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
        "        'Linear Regression': LinearRegression()\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'r2': r2,\n",
        "            'rmse': rmse,\n",
        "            'y_pred': y_pred,\n",
        "            'y_test': y_test\n",
        "        }\n",
        "        \n",
        "        print(f\"\\\\n{name}:\")\n",
        "        print(f\"  R²: {r2:.3f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "    \n",
        "    # Feature importance (from Random Forest)\n",
        "    if 'Random Forest' in results:\n",
        "        rf_model = results['Random Forest']['model']\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(f\"\\\\nTop 10 most predictive architectural features:\")\n",
        "        print(feature_importance.head(10).to_string(index=False))\n",
        "        \n",
        "        # Visualize feature importance\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Feature importance\n",
        "        top_features = feature_importance.head(15)\n",
        "        axes[0,0].barh(range(len(top_features)), top_features['importance'], color='steelblue', alpha=0.7)\n",
        "        axes[0,0].set_yticks(range(len(top_features)))\n",
        "        axes[0,0].set_yticklabels(top_features['feature'], fontsize=9)\n",
        "        axes[0,0].invert_yaxis()\n",
        "        axes[0,0].set_xlabel('Feature Importance', fontsize=11)\n",
        "        axes[0,0].set_title('Top 15 Most Predictive Config Features for Elo', fontsize=13)\n",
        "        axes[0,0].grid(True, alpha=0.3, axis='x')\n",
        "        \n",
        "        # 2. Predicted vs Actual (Random Forest)\n",
        "        axes[0,1].scatter(results['Random Forest']['y_test'], results['Random Forest']['y_pred'], \n",
        "                         alpha=0.5, s=20, color='steelblue', edgecolors='black', linewidth=0.3)\n",
        "        min_val = min(results['Random Forest']['y_test'].min(), results['Random Forest']['y_pred'].min())\n",
        "        max_val = max(results['Random Forest']['y_test'].max(), results['Random Forest']['y_pred'].max())\n",
        "        axes[0,1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
        "        axes[0,1].set_xlabel('Actual Elo', fontsize=11)\n",
        "        axes[0,1].set_ylabel('Predicted Elo', fontsize=11)\n",
        "        axes[0,1].set_title(f'Predicted vs Actual Elo (R²={results[\\\"Random Forest\\\"][\\\"r2\\\"]:.3f})', fontsize=13)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Model comparison\n",
        "        model_names = list(results.keys())\n",
        "        r2_scores = [results[m]['r2'] for m in model_names]\n",
        "        axes[1,0].bar(range(len(model_names)), r2_scores, color=['steelblue', 'coral', 'seagreen'], alpha=0.7)\n",
        "        axes[1,0].set_xticks(range(len(model_names)))\n",
        "        axes[1,0].set_xticklabels(model_names)\n",
        "        axes[1,0].set_ylabel('R² Score', fontsize=11)\n",
        "        axes[1,0].set_title('Model Performance Comparison', fontsize=13)\n",
        "        axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "        for i, score in enumerate(r2_scores):\n",
        "            axes[1,0].text(i, score + 0.01, f'{score:.3f}', ha='center', fontsize=10)\n",
        "        \n",
        "        # 4. Residuals plot\n",
        "        residuals = results['Random Forest']['y_test'] - results['Random Forest']['y_pred']\n",
        "        axes[1,1].scatter(results['Random Forest']['y_pred'], residuals, alpha=0.5, s=20, color='coral', edgecolors='black', linewidth=0.3)\n",
        "        axes[1,1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "        axes[1,1].set_xlabel('Predicted Elo', fontsize=11)\n",
        "        axes[1,1].set_ylabel('Residuals', fontsize=11)\n",
        "        axes[1,1].set_title('Residuals Plot', fontsize=13)\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/architecture_capability_regression.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save feature importance\n",
        "        feature_importance.to_csv('config_feature_importance_elo.csv', index=False)\n",
        "        print(\"\\\\n✓ Feature importance saved to config_feature_importance_elo.csv\")\n",
        "    else:\n",
        "        print(\"Random Forest model not available for feature importance\")\n",
        "else:\n",
        "    print(\"Not enough Elo data for regression analysis (need >50 models)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Architecture vs Behavioral Clusters\n",
        "\n",
        "**Goal**: Compare clustering based on architecture (config) vs clustering based on behavior (Elo/capability).\n",
        "\n",
        "**Key Questions**:\n",
        "- Do architecturally similar models behave similarly?\n",
        "- Are families behaviorally monomorphic or polymorphic?\n",
        "- Do architecture clusters align with behavioral clusters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare architecture-based vs behavior-based clustering\n",
        "if 'elo_rating' in df.columns and df['elo_rating'].notna().sum() > 50:\n",
        "    print(\"Comparing architecture-based vs behavior-based clustering...\")\n",
        "    \n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "    \n",
        "    # Prepare data with both config and Elo\n",
        "    df_cluster = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    if len(df_cluster) > 100:\n",
        "        # Sample for computational efficiency\n",
        "        df_cluster = df_cluster.sample(n=min(1000, len(df_cluster)), random_state=42)\n",
        "    \n",
        "    # Architecture features (config-based)\n",
        "    arch_features = [col for col in numeric_features + boolean_feature_list if col in df_cluster.columns]\n",
        "    X_arch = df_cluster[arch_features].fillna(0)\n",
        "    X_arch = X_arch.loc[:, X_arch.std() > 0]  # Remove zero-variance columns\n",
        "    \n",
        "    # Standardize\n",
        "    scaler_arch = StandardScaler()\n",
        "    X_arch_scaled = scaler_arch.fit_transform(X_arch)\n",
        "    \n",
        "    # Behavioral features (Elo-based, can extend to multi-dimensional)\n",
        "    X_behavior = df_cluster[['elo_rating']].values\n",
        "    \n",
        "    # Cluster architectures\n",
        "    n_clusters = min(10, len(df_cluster) // 20)\n",
        "    kmeans_arch = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    arch_clusters = kmeans_arch.fit_predict(X_arch_scaled)\n",
        "    \n",
        "    # Cluster behaviors (by Elo bins)\n",
        "    behavior_clusters = pd.cut(df_cluster['elo_rating'], bins=n_clusters, labels=False)\n",
        "    \n",
        "    # Compute cluster alignment\n",
        "    ari = adjusted_rand_score(arch_clusters, behavior_clusters)\n",
        "    nmi = normalized_mutual_info_score(arch_clusters, behavior_clusters)\n",
        "    \n",
        "    print(f\"\\\\nCluster Alignment Metrics:\")\n",
        "    print(f\"  Adjusted Rand Index: {ari:.3f} (1.0 = perfect alignment, 0.0 = random)\")\n",
        "    print(f\"  Normalized Mutual Information: {nmi:.3f} (1.0 = perfect alignment, 0.0 = independent)\")\n",
        "    \n",
        "    # Visualize clusters\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Use t-SNE for 2D embedding\n",
        "    print(\"\\\\nComputing t-SNE embeddings (this may take a minute)...\")\n",
        "    tsne_arch = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df_cluster)-1))\n",
        "    embedding_arch = tsne_arch.fit_transform(X_arch_scaled)\n",
        "    \n",
        "    # 1. Architecture clusters colored by cluster\n",
        "    scatter1 = axes[0,0].scatter(embedding_arch[:, 0], embedding_arch[:, 1], \n",
        "                                c=arch_clusters, cmap='tab10', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
        "    axes[0,0].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
        "    axes[0,0].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
        "    axes[0,0].set_title('Architecture-Based Clusters (Config Features)', fontsize=13)\n",
        "    plt.colorbar(scatter1, ax=axes[0,0])\n",
        "    \n",
        "    # 2. Architecture embedding colored by Elo\n",
        "    scatter2 = axes[0,1].scatter(embedding_arch[:, 0], embedding_arch[:, 1], \n",
        "                                c=df_cluster['elo_rating'], cmap='viridis', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
        "    axes[0,1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
        "    axes[0,1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
        "    axes[0,1].set_title('Architecture Space Colored by Elo (Behavior)', fontsize=13)\n",
        "    plt.colorbar(scatter2, ax=axes[0,1], label='Elo Rating')\n",
        "    \n",
        "    # 3. Elo distribution by architecture cluster\n",
        "    cluster_elo = pd.DataFrame({\n",
        "        'cluster': arch_clusters,\n",
        "        'elo': df_cluster['elo_rating'].values\n",
        "    })\n",
        "    cluster_elo_stats = cluster_elo.groupby('cluster')['elo'].agg(['mean', 'std', 'count'])\n",
        "    \n",
        "    x_pos = np.arange(len(cluster_elo_stats))\n",
        "    axes[1,0].bar(x_pos, cluster_elo_stats['mean'], yerr=cluster_elo_stats['std'], \n",
        "                  color='steelblue', alpha=0.7, capsize=5)\n",
        "    axes[1,0].set_xticks(x_pos)\n",
        "    axes[1,0].set_xticklabels([f'Cluster {i}' for i in cluster_elo_stats.index])\n",
        "    axes[1,0].set_ylabel('Mean Elo Rating', fontsize=11)\n",
        "    axes[1,0].set_title('Behavioral Capability by Architecture Cluster', fontsize=13)\n",
        "    axes[1,0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add count labels\n",
        "    for i, (idx, row) in enumerate(cluster_elo_stats.iterrows()):\n",
        "        axes[1,0].text(i, row['mean'] + row['std'] + 5, f\\\"n={int(row['count'])}\\\", ha='center', fontsize=8)\n",
        "    \n",
        "    # 4. Cluster alignment confusion matrix\n",
        "    confusion = pd.crosstab(pd.Series(arch_clusters, name='Architecture Cluster'),\n",
        "                           pd.Series(behavior_clusters, name='Behavior Cluster'))\n",
        "    im = axes[1,1].imshow(confusion.values, cmap='YlOrRd', aspect='auto')\n",
        "    axes[1,1].set_xticks(range(len(confusion.columns)))\n",
        "    axes[1,1].set_xticklabels([f'B{i}' for i in confusion.columns])\n",
        "    axes[1,1].set_yticks(range(len(confusion.index)))\n",
        "    axes[1,1].set_yticklabels([f'A{i}' for i in confusion.index])\n",
        "    axes[1,1].set_xlabel('Behavior Cluster', fontsize=11)\n",
        "    axes[1,1].set_ylabel('Architecture Cluster', fontsize=11)\n",
        "    axes[1,1].set_title(f'Architecture vs Behavior Cluster Alignment\\\\n(ARI={ari:.3f}, NMI={nmi:.3f})', fontsize=13)\n",
        "    plt.colorbar(im, ax=axes[1,1], label='Count')\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(confusion.index)):\n",
        "        for j in range(len(confusion.columns)):\n",
        "            axes[1,1].text(j, i, str(confusion.iloc[i, j]), ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('figures/architecture_vs_behavior_clusters.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\n✓ Cluster analysis complete\")\n",
        "else:\n",
        "    print(\"Not enough behavioral data for cluster comparison\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Ecosystem Fitness vs Behavioral Fitness\n",
        "\n",
        "**Goal**: Compare ecosystem success metrics (downloads, descendants, likes) with behavioral capability (Elo).\n",
        "\n",
        "**Key Questions**:\n",
        "- Are the most downloaded models the most capable?\n",
        "- Do high-performing models have more descendants?\n",
        "- What is the relationship between ecosystem adoption and behavioral performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare ecosystem fitness vs behavioral fitness\n",
        "# Try to load ecosystem metrics (downloads, likes, descendants)\n",
        "print(\"Analyzing ecosystem fitness vs behavioral fitness...\")\n",
        "\n",
        "# Check for ecosystem metrics in dataframe\n",
        "ecosystem_metrics = {}\n",
        "for metric in ['downloads', 'likes', 'num_descendants', 'num_children']:\n",
        "    if metric in df.columns:\n",
        "        ecosystem_metrics[metric] = df[metric]\n",
        "\n",
        "# Also try loading from graph if available\n",
        "if G_family is not None and 'elo_rating' in df.columns:\n",
        "    # Count descendants for models with Elo\n",
        "    df_with_descendants = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    def count_descendants_simple(model_id):\n",
        "        try:\n",
        "            descendants = nx.descendants(G_family, model_id)\n",
        "            return len(descendants)\n",
        "        except:\n",
        "            return 0\n",
        "    \n",
        "    # Sample for performance\n",
        "    if len(df_with_descendants) > 500:\n",
        "        df_with_descendants = df_with_descendants.sample(n=500, random_state=42)\n",
        "    \n",
        "    df_with_descendants['num_descendants'] = df_with_descendants['modelId'].apply(count_descendants_simple)\n",
        "    ecosystem_metrics['num_descendants'] = df_with_descendants.set_index('modelId')['num_descendants']\n",
        "\n",
        "if len(ecosystem_metrics) > 0 and 'elo_rating' in df.columns:\n",
        "    # Merge ecosystem metrics with Elo\n",
        "    df_fitness = df[df['elo_rating'].notna()].copy()\n",
        "    \n",
        "    for metric_name, metric_series in ecosystem_metrics.items():\n",
        "        if isinstance(metric_series, pd.Series):\n",
        "            df_fitness = df_fitness.merge(\n",
        "                metric_series.reset_index().rename(columns={metric_series.name: metric_name}),\n",
        "                left_on='modelId',\n",
        "                right_on=metric_series.index.name if metric_series.index.name else 'index',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_fitness[metric_name] = df_fitness['modelId'].map(metric_series).fillna(0)\n",
        "    \n",
        "    # Filter to models with both Elo and at least one ecosystem metric\n",
        "    has_metric = df_fitness[[m for m in ecosystem_metrics.keys()]].notna().any(axis=1)\n",
        "    df_fitness = df_fitness[has_metric].copy()\n",
        "    \n",
        "    if len(df_fitness) > 10:\n",
        "        print(f\"\\\\n✓ Analyzing {len(df_fitness):,} models with both Elo and ecosystem metrics\")\n",
        "        \n",
        "        # Visualize relationships\n",
        "        n_metrics = len(ecosystem_metrics)\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Scatter: Downloads vs Elo\n",
        "        if 'downloads' in df_fitness.columns:\n",
        "            # Log scale for downloads\n",
        "            log_downloads = np.log10(df_fitness['downloads'] + 1)\n",
        "            axes[0,0].scatter(log_downloads, df_fitness['elo_rating'], alpha=0.5, s=30, \n",
        "                            color='steelblue', edgecolors='black', linewidth=0.3)\n",
        "            axes[0,0].set_xlabel('Log10(Downloads + 1)', fontsize=11)\n",
        "            axes[0,0].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[0,0].set_title('Ecosystem Adoption (Downloads) vs Behavioral Capability', fontsize=13)\n",
        "            axes[0,0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add correlation\n",
        "            corr, p_val = pearsonr(log_downloads, df_fitness['elo_rating'])\n",
        "            axes[0,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                          transform=axes[0,0].transAxes, fontsize=10,\n",
        "                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 2. Quadrant plot: Ecosystem Fitness vs Behavioral Fitness\n",
        "        if 'downloads' in df_fitness.columns:\n",
        "            median_downloads = df_fitness['downloads'].median()\n",
        "            median_elo = df_fitness['elo_rating'].median()\n",
        "            \n",
        "            quadrants = {\n",
        "                'High Adoption, High Capability': (df_fitness['downloads'] > median_downloads) & (df_fitness['elo_rating'] > median_elo),\n",
        "                'High Adoption, Low Capability': (df_fitness['downloads'] > median_downloads) & (df_fitness['elo_rating'] <= median_elo),\n",
        "                'Low Adoption, High Capability': (df_fitness['downloads'] <= median_downloads) & (df_fitness['elo_rating'] > median_elo),\n",
        "                'Low Adoption, Low Capability': (df_fitness['downloads'] <= median_downloads) & (df_fitness['elo_rating'] <= median_elo)\n",
        "            }\n",
        "            \n",
        "            colors = {'High Adoption, High Capability': 'green', 'High Adoption, Low Capability': 'orange',\n",
        "                     'Low Adoption, High Capability': 'blue', 'Low Adoption, Low Capability': 'red'}\n",
        "            \n",
        "            for quad_name, mask in quadrants.items():\n",
        "                if mask.sum() > 0:\n",
        "                    axes[0,1].scatter(np.log10(df_fitness[mask]['downloads'] + 1), \n",
        "                                     df_fitness[mask]['elo_rating'],\n",
        "                                     alpha=0.5, s=30, label=quad_name, c=colors[quad_name], \n",
        "                                     edgecolors='black', linewidth=0.3)\n",
        "            \n",
        "            axes[0,1].axhline(median_elo, color='gray', linestyle='--', alpha=0.5)\n",
        "            axes[0,1].axvline(np.log10(median_downloads + 1), color='gray', linestyle='--', alpha=0.5)\n",
        "            axes[0,1].set_xlabel('Log10(Downloads + 1)', fontsize=11)\n",
        "            axes[0,1].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[0,1].set_title('Ecosystem Fitness vs Behavioral Fitness Quadrants', fontsize=13)\n",
        "            axes[0,1].legend(fontsize=8)\n",
        "            axes[0,1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Descendants vs Elo\n",
        "        if 'num_descendants' in df_fitness.columns:\n",
        "            log_descendants = np.log10(df_fitness['num_descendants'] + 1)\n",
        "            axes[1,0].scatter(log_descendants, df_fitness['elo_rating'], alpha=0.5, s=30,\n",
        "                            color='coral', edgecolors='black', linewidth=0.3)\n",
        "            axes[1,0].set_xlabel('Log10(Number of Descendants + 1)', fontsize=11)\n",
        "            axes[1,0].set_ylabel('Elo Rating', fontsize=11)\n",
        "            axes[1,0].set_title('Reproductive Success vs Behavioral Capability', fontsize=13)\n",
        "            axes[1,0].grid(True, alpha=0.3)\n",
        "            \n",
        "            corr, p_val = pearsonr(log_descendants, df_fitness['elo_rating'])\n",
        "            axes[1,0].text(0.05, 0.95, f'r={corr:.3f}, p={p_val:.3f}', \n",
        "                          transform=axes[1,0].transAxes, fontsize=10,\n",
        "                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 4. Correlation matrix\n",
        "        fitness_cols = ['elo_rating'] + [m for m in ecosystem_metrics.keys() if m in df_fitness.columns]\n",
        "        fitness_data = df_fitness[fitness_cols].copy()\n",
        "        \n",
        "        # Log transform ecosystem metrics\n",
        "        for col in fitness_data.columns:\n",
        "            if col != 'elo_rating' and fitness_data[col].max() > 100:\n",
        "                fitness_data[col] = np.log10(fitness_data[col] + 1)\n",
        "        \n",
        "        corr_matrix = fitness_data.corr()\n",
        "        im = axes[1,1].imshow(corr_matrix.values, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "        axes[1,1].set_xticks(range(len(corr_matrix.columns)))\n",
        "        axes[1,1].set_yticks(range(len(corr_matrix.index)))\n",
        "        axes[1,1].set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=9)\n",
        "        axes[1,1].set_yticklabels(corr_matrix.index, fontsize=9)\n",
        "        axes[1,1].set_title('Correlation: Ecosystem Metrics vs Behavioral Capability', fontsize=13)\n",
        "        plt.colorbar(im, ax=axes[1,1], label='Correlation')\n",
        "        \n",
        "        # Add correlation values\n",
        "        for i in range(len(corr_matrix.index)):\n",
        "            for j in range(len(corr_matrix.columns)):\n",
        "                axes[1,1].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
        "                              ha='center', va='center', fontsize=8,\n",
        "                              color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('figures/ecosystem_vs_behavioral_fitness.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Save fitness data\n",
        "        df_fitness[['modelId', 'elo_rating'] + list(ecosystem_metrics.keys())].to_csv('ecosystem_behavioral_fitness.csv', index=False)\n",
        "        print(\"\\\\n✓ Fitness data saved to ecosystem_behavioral_fitness.csv\")\n",
        "    else:\n",
        "        print(\"Not enough data for fitness analysis\")\n",
        "else:\n",
        "    print(\"Ecosystem metrics or Elo data not available - skipping fitness analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Final Summary: Config Similarity Analysis\n",
        "\n",
        "This notebook implements a comprehensive config-based similarity analysis, replicating the AI Ecosystem paper methodology but grounded in architectural similarity rather than metadata traits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CONFIG SIMILARITY ANALYSIS - COMPREHENSIVE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\\\n1. SIMILARITY GRAPH:\")\n",
        "print(f\"   - Nodes: {len(G_similarity.nodes):,}\")\n",
        "print(f\"   - Edges: {len(G_similarity.edges):,}\")\n",
        "print(f\"   - Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
        "\n",
        "if len(df_drift) > 0:\n",
        "    print(f\"\\\\n2. CONFIG DRIFT ANALYSIS:\")\n",
        "    print(f\"   - Parent-child pairs analyzed: {len(df_drift):,}\")\n",
        "    print(f\"   - Mean drift: {df_drift['drift'].mean():.3f}\")\n",
        "    print(f\"   - Median drift: {df_drift['drift'].median():.3f}\")\n",
        "    if 'same_family' in df_drift.columns:\n",
        "        within = df_drift[df_drift['same_family'] == True]['drift'].mean() if df_drift['same_family'].sum() > 0 else None\n",
        "        between = df_drift[df_drift['same_family'] == False]['drift'].mean() if (df_drift['same_family'] == False).sum() > 0 else None\n",
        "        if within is not None:\n",
        "            print(f\"   - Within-family drift: {within:.3f}\")\n",
        "        if between is not None:\n",
        "            print(f\"   - Between-family drift: {between:.3f}\")\n",
        "\n",
        "if len(df_mutations) > 0:\n",
        "    print(f\"\\\\n3. MUTATIONAL LANDSCAPE:\")\n",
        "    print(f\"   - Features analyzed: {len(df_mutations)}\")\n",
        "    print(f\"   - Most mutated feature: {df_mutations.iloc[0]['feature']} (rate: {df_mutations.iloc[0]['mutation_rate']:.2%})\")\n",
        "    print(f\"   - Mean mutation rate: {df_mutations['mutation_rate'].mean():.2%}\")\n",
        "\n",
        "if 'df_subgraph_stats' in locals() and len(df_subgraph_stats) > 0:\n",
        "    print(f\"\\\\n4. SUBGRAPH ANALYSIS:\")\n",
        "    print(f\"   - Subgraphs analyzed: {len(df_subgraph_stats)}\")\n",
        "    print(f\"   - Mean subgraph similarity: {df_subgraph_stats['mean_similarity'].mean():.3f}\")\n",
        "\n",
        "if 'df_depth_drift' in locals() and len(df_depth_drift) > 0:\n",
        "    print(f\"\\\\n5. DEPTH ANALYSIS:\")\n",
        "    print(f\"   - Lineage paths analyzed: {df_depth_drift['root_id'].nunique()}\")\n",
        "    print(f\"   - Max depth analyzed: {df_depth_drift['depth'].max()}\")\n",
        "\n",
        "print(f\"\\\\n6. OUTPUT FILES GENERATED:\")\n",
        "output_files = [\n",
        "    'config_similarity_summary.csv',\n",
        "    'config_drift_pairs.csv',\n",
        "    'config_mutation_rates.csv'\n",
        "]\n",
        "if 'df_subgraph_stats' in locals():\n",
        "    output_files.append('subgraph_similarity_stats.csv')\n",
        "if 'df_depth_drift' in locals():\n",
        "    output_files.append('config_drift_by_depth.csv')\n",
        "\n",
        "for f in output_files:\n",
        "    print(f\"   - {f}\")\n",
        "\n",
        "print(f\"\\\\n7. FIGURES GENERATED:\")\n",
        "figures = [\n",
        "    'figures/config_similarity_embedding.png',\n",
        "    'figures/similarity_metrics_comparison.png',\n",
        "    'figures/config_drift_analysis.png',\n",
        "    'figures/mutational_landscape.png'\n",
        "]\n",
        "if 'df_subgraph_stats' in locals():\n",
        "    figures.append('figures/subgraph_similarity_analysis.png')\n",
        "if 'df_depth_drift' in locals():\n",
        "    figures.append('figures/drift_by_depth.png')\n",
        "\n",
        "for f in figures:\n",
        "    print(f\"   - {f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute Gower distance matrix\n",
        "print(\"Computing Gower distance matrix (this may take a few minutes)...\")\n",
        "\n",
        "n = len(df_sample)\n",
        "distance_matrix = np.zeros((n, n))\n",
        "\n",
        "# Prepare feature vectors\n",
        "feature_vectors = df_sample[available_features].copy()\n",
        "\n",
        "# Compute pairwise distances\n",
        "for i in range(n):\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  Processing {i}/{n}...\")\n",
        "    for j in range(i+1, n):\n",
        "        dist = gower_distance(\n",
        "            feature_vectors.iloc[i],\n",
        "            feature_vectors.iloc[j],\n",
        "            numeric_features,\n",
        "            categorical_features,\n",
        "            boolean_feature_list\n",
        "        )\n",
        "        distance_matrix[i, j] = dist\n",
        "        distance_matrix[j, i] = dist  # Symmetric\n",
        "\n",
        "print(f\"✓ Distance matrix computed: {distance_matrix.shape}\")\n",
        "print(f\"Distance range: [{distance_matrix[distance_matrix > 0].min():.3f}, {distance_matrix.max():.3f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Similarity Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create similarity graph\n",
        "# Edge threshold: connect models with similarity > threshold\n",
        "# Lower distance = higher similarity, so we use distance < threshold\n",
        "\n",
        "SIMILARITY_THRESHOLD = 0.3  # Connect models with Gower distance < 0.3 (70%+ similar)\n",
        "\n",
        "G_similarity = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for idx, row in df_sample.iterrows():\n",
        "    model_id = row['modelId']\n",
        "    G_similarity.add_node(model_id, **row.to_dict())\n",
        "\n",
        "# Add edges for similar models\n",
        "edge_count = 0\n",
        "for i in range(n):\n",
        "    model_i = df_sample.iloc[i]['modelId']\n",
        "    for j in range(i+1, n):\n",
        "        if distance_matrix[i, j] < SIMILARITY_THRESHOLD:\n",
        "            model_j = df_sample.iloc[j]['modelId']\n",
        "            G_similarity.add_edge(model_i, model_j, weight=1 - distance_matrix[i, j], distance=distance_matrix[i, j])\n",
        "            edge_count += 1\n",
        "\n",
        "print(f\"✓ Similarity graph created\")\n",
        "print(f\"  Nodes: {len(G_similarity.nodes):,}\")\n",
        "print(f\"  Edges: {len(G_similarity.edges):,}\")\n",
        "print(f\"  Average degree: {2 * len(G_similarity.edges) / len(G_similarity.nodes):.2f}\")\n",
        "print(f\"  Connected components: {nx.number_connected_components(G_similarity)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Graph Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Graph statistics\n",
        "if len(G_similarity.nodes) > 0:\n",
        "    # Largest connected component\n",
        "    largest_cc = max(nx.connected_components(G_similarity), key=len)\n",
        "    G_largest = G_similarity.subgraph(largest_cc)\n",
        "    \n",
        "    print(f\"Largest connected component: {len(largest_cc):,} nodes\")\n",
        "    print(f\"\\nGraph statistics:\")\n",
        "    print(f\"  Average clustering coefficient: {nx.average_clustering(G_similarity):.3f}\")\n",
        "    print(f\"  Density: {nx.density(G_similarity):.6f}\")\n",
        "    \n",
        "    # Degree distribution\n",
        "    degrees = dict(G_similarity.degree())\n",
        "    print(f\"\\nDegree distribution:\")\n",
        "    print(f\"  Min degree: {min(degrees.values())}\")\n",
        "    print(f\"  Max degree: {max(degrees.values())}\")\n",
        "    print(f\"  Mean degree: {np.mean(list(degrees.values())):.2f}\")\n",
        "    print(f\"  Median degree: {np.median(list(degrees.values())):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Similarity Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use PCA to reduce dimensions for visualization\n",
        "# Convert distance matrix to similarity matrix\n",
        "similarity_matrix = 1 - distance_matrix\n",
        "\n",
        "# Use MDS or PCA on similarity matrix\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "# For visualization, use 2D embedding\n",
        "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
        "embedding = mds.fit_transform(distance_matrix)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Color by family if available\n",
        "if 'family' in df_sample.columns:\n",
        "    families = df_sample['family'].values\n",
        "    unique_families = pd.Series(families).unique()\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_families)))\n",
        "    family_color_map = {fam: colors[i] for i, fam in enumerate(unique_families)}\n",
        "    node_colors = [family_color_map.get(fam, 'gray') for fam in families]\n",
        "else:\n",
        "    node_colors = 'steelblue'\n",
        "\n",
        "# Plot 1: Full similarity graph embedding\n",
        "scatter = axes[0].scatter(embedding[:, 0], embedding[:, 1], c=node_colors, alpha=0.6, s=20)\n",
        "axes[0].set_xlabel('MDS Dimension 1', fontsize=11)\n",
        "axes[0].set_ylabel('MDS Dimension 2', fontsize=11)\n",
        "axes[0].set_title(f'Config Similarity Embedding (n={len(df_sample):,})', fontsize=13)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add edges for top similarities (to avoid clutter)\n",
        "if len(G_similarity.edges) > 0:\n",
        "    # Sample edges for visualization\n",
        "    edge_sample = list(G_similarity.edges(data=True))[:min(500, len(G_similarity.edges))]\n",
        "    \n",
        "    for edge in edge_sample:\n",
        "        model_i, model_j, data = edge\n",
        "        idx_i = df_sample[df_sample['modelId'] == model_i].index[0] if len(df_sample[df_sample['modelId'] == model_i]) > 0 else None\n",
        "        idx_j = df_sample[df_sample['modelId'] == model_j].index[0] if len(df_sample[df_sample['modelId'] == model_j]) > 0 else None\n",
        "        \n",
        "        if idx_i is not None and idx_j is not None:\n",
        "            i_pos = df_sample.index.get_loc(idx_i) if idx_i in df_sample.index else None\n",
        "            j_pos = df_sample.index.get_loc(idx_j) if idx_j in df_sample.index else None\n",
        "            \n",
        "            if i_pos is not None and j_pos is not None and i_pos < len(embedding) and j_pos < len(embedding):\n",
        "                axes[0].plot([embedding[i_pos, 0], embedding[j_pos, 0]], \n",
        "                           [embedding[i_pos, 1], embedding[j_pos, 1]], \n",
        "                           'gray', alpha=0.1, linewidth=0.5)\n",
        "\n",
        "# Plot 2: Degree distribution\n",
        "degrees = dict(G_similarity.degree())\n",
        "degree_values = list(degrees.values())\n",
        "axes[1].hist(degree_values, bins=30, color='steelblue', alpha=0.7, edgecolor='white')\n",
        "axes[1].set_xlabel('Node Degree (Number of Similar Models)', fontsize=11)\n",
        "axes[1].set_ylabel('Count', fontsize=11)\n",
        "axes[1].set_title('Similarity Graph Degree Distribution', fontsize=13)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/config_similarity_embedding.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Find Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find nearest neighbors for a sample of models\n",
        "def get_nearest_neighbors(model_id, k=5):\n",
        "    \"\"\"Get k nearest neighbors for a given model\"\"\"\n",
        "    if model_id not in df_sample['modelId'].values:\n",
        "        return None\n",
        "    \n",
        "    model_idx = df_sample[df_sample['modelId'] == model_id].index[0]\n",
        "    model_pos = df_sample.index.get_loc(model_idx)\n",
        "    \n",
        "    # Get distances to all other models\n",
        "    distances = distance_matrix[model_pos, :]\n",
        "    \n",
        "    # Get k+1 nearest (including self)\n",
        "    nearest_indices = np.argsort(distances)[:k+1]\n",
        "    \n",
        "    neighbors = []\n",
        "    for idx in nearest_indices[1:]:  # Skip self\n",
        "        neighbor_id = df_sample.iloc[idx]['modelId']\n",
        "        distance = distances[idx]\n",
        "        neighbors.append({\n",
        "            'modelId': neighbor_id,\n",
        "            'distance': distance,\n",
        "            'similarity': 1 - distance\n",
        "        })\n",
        "    \n",
        "    return neighbors\n",
        "\n",
        "# Example: Find nearest neighbors for a few models\n",
        "sample_models = df_sample['modelId'].head(5).tolist()\n",
        "\n",
        "print(\"Nearest neighbors examples:\\n\")\n",
        "for model_id in sample_models[:3]:\n",
        "    neighbors = get_nearest_neighbors(model_id, k=5)\n",
        "    if neighbors:\n",
        "        print(f\"\\nModel: {model_id}\")\n",
        "        print(f\"  Nearest neighbors:\")\n",
        "        for i, neighbor in enumerate(neighbors, 1):\n",
        "            print(f\"    {i}. {neighbor['modelId']} (similarity: {neighbor['similarity']:.3f}, distance: {neighbor['distance']:.3f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare Similarity Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare different similarity metrics on a small sample\n",
        "COMPARE_SAMPLE = 100\n",
        "df_compare = df_sample.head(COMPARE_SAMPLE).copy()\n",
        "\n",
        "# Prepare numeric features only for L2/L1/cosine\n",
        "numeric_data = df_compare[numeric_features].copy()\n",
        "numeric_data = numeric_data.fillna(0)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "numeric_scaled = scaler.fit_transform(numeric_data)\n",
        "\n",
        "# Compute different distance metrics\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "l2_distances = squareform(pdist(numeric_scaled, metric='euclidean'))\n",
        "l1_distances = squareform(pdist(numeric_scaled, metric='cityblock'))\n",
        "cosine_distances = squareform(pdist(numeric_scaled, metric='cosine'))\n",
        "\n",
        "# Compare with Gower (on same sample)\n",
        "gower_sample = distance_matrix[:COMPARE_SAMPLE, :COMPARE_SAMPLE]\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "metrics = [\n",
        "    ('Gower Distance', gower_sample, 'viridis'),\n",
        "    ('L2 (Euclidean)', l2_distances / l2_distances.max(), 'plasma'),\n",
        "    ('L1 (Manhattan)', l1_distances / l1_distances.max(), 'magma'),\n",
        "    ('Cosine Distance', cosine_distances, 'cividis')\n",
        "]\n",
        "\n",
        "for idx, (name, dist_mat, cmap) in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    im = ax.imshow(dist_mat, cmap=cmap, aspect='auto', interpolation='nearest')\n",
        "    ax.set_title(f'{name} Matrix (n={COMPARE_SAMPLE})', fontsize=11)\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/similarity_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSimilarity metrics comparison:\")\n",
        "print(f\"  Gower: range [{gower_sample[gower_sample > 0].min():.3f}, {gower_sample.max():.3f}]\")\n",
        "print(f\"  L2: range [{l2_distances[l2_distances > 0].min():.3f}, {l2_distances.max():.3f}]\")\n",
        "print(f\"  L1: range [{l1_distances[l1_distances > 0].min():.3f}, {l1_distances.max():.3f}]\")\n",
        "print(f\"  Cosine: range [{cosine_distances[cosine_distances > 0].min():.3f}, {cosine_distances.max():.3f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CONFIG SIMILARITY GRAPH SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Total models analyzed: {len(df_sample):,}\")\n",
        "print(f\"  Features used: {len(available_features)}\")\n",
        "print(f\"    - Numeric: {len(numeric_features)}\")\n",
        "print(f\"    - Categorical: {len(categorical_features)}\")\n",
        "print(f\"    - Boolean: {len(boolean_feature_list)}\")\n",
        "\n",
        "print(f\"\\nSimilarity Graph:\")\n",
        "print(f\"  Nodes: {len(G_similarity.nodes):,}\")\n",
        "print(f\"  Edges: {len(G_similarity.edges):,}\")\n",
        "print(f\"  Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"  Average degree: {2 * len(G_similarity.edges) / len(G_similarity.nodes):.2f}\")\n",
        "print(f\"  Connected components: {nx.number_connected_components(G_similarity)}\")\n",
        "\n",
        "if len(G_similarity.nodes) > 0:\n",
        "    print(f\"  Average clustering: {nx.average_clustering(G_similarity):.3f}\")\n",
        "    print(f\"  Graph density: {nx.density(G_similarity):.6f}\")\n",
        "\n",
        "print(f\"\\nDistance Statistics:\")\n",
        "print(f\"  Mean distance: {distance_matrix[distance_matrix > 0].mean():.3f}\")\n",
        "print(f\"  Median distance: {np.median(distance_matrix[distance_matrix > 0]):.3f}\")\n",
        "print(f\"  Min distance: {distance_matrix[distance_matrix > 0].min():.3f}\")\n",
        "print(f\"  Max distance: {distance_matrix.max():.3f}\")\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'n_models': len(df_sample),\n",
        "    'n_features': len(available_features),\n",
        "    'n_nodes': len(G_similarity.nodes),\n",
        "    'n_edges': len(G_similarity.edges),\n",
        "    'similarity_threshold': SIMILARITY_THRESHOLD,\n",
        "    'mean_distance': float(distance_matrix[distance_matrix > 0].mean()),\n",
        "    'median_distance': float(np.median(distance_matrix[distance_matrix > 0]))\n",
        "}\n",
        "\n",
        "pd.DataFrame([summary]).to_csv('config_similarity_summary.csv', index=False)\n",
        "print(\"\\n✓ Summary saved to config_similarity_summary.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}